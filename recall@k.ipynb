{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e837dfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "VER = 6\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import os, sys, pickle, glob, gc\n",
    "from collections import Counter\n",
    "import cudf, itertools\n",
    "print('We will use RAPIDS version',cudf.__version__)\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import polars as pl\n",
    "\n",
    "from pyarrow.parquet import ParquetFile\n",
    "import pyarrow as pa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda95d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_FOR = \"local\"\n",
    "CANDIDATE_COUNT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4778e5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTE METRIC\n",
    "score = 0\n",
    "weights = {'clicks': 0.10, 'carts': 0.30, 'orders': 0.60}\n",
    "for t in ['clicks','carts','orders']:\n",
    "    idf = pd.read_parquet(f\"./candidate_data/{GENERATE_FOR}_{CANDIDATE_COUNT}candidates_{t}.parquet\")\n",
    "    sub = idf.groupby('session').aid.progress_apply(list)\n",
    "    sub = sub.to_frame().reset_index()\n",
    "    sub.item = sub.aid.apply(lambda x: \" \".join(map(str,x)))\n",
    "    sub.columns = ['session_type','labels']\n",
    "    sub.session_type = sub.session_type.astype('str') + '_' + t\n",
    "    sub['session'] = sub.session_type.apply(lambda x: int(x.split('_')[0]))\n",
    "#     sub.labels = sub.labels.apply(lambda x: [int(i) for i in x.split(' ')[:20]])\n",
    "    test_labels = pd.read_parquet('./splitted_raw_data/val_labels.parquet')\n",
    "    test_labels = test_labels.loc[test_labels['type']==t]\n",
    "    test_labels = test_labels.merge(sub, how='left', on=['session'])\n",
    "    test_labels['hits'] = test_labels.apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n",
    "    test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0,20)\n",
    "    recall = test_labels['hits'].sum() / test_labels['gt_count'].sum()\n",
    "    score += weights[t]*recall\n",
    "    print(f'{t} recall =',recall)\n",
    "\n",
    "print('=============')\n",
    "print('Overall Recall =',score)\n",
    "print('=============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db79a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
