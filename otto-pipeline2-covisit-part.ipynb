{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Step 1 - Candidate Generation with RAPIDS\nFor candidate generation, we build three co-visitation matrices. One computes the popularity of cart/order given a user's previous click/cart/order. We apply type weighting to this matrix. One computes the popularity of cart/order given a user's previous cart/order. We call this \"buy2buy\" matrix. One computes the popularity of clicks given a user previously click/cart/order.  We apply time weighting to this matrix. We will use RAPIDS cuDF GPU to compute these matrices quickly!","metadata":{"papermill":{"duration":0.00373,"end_time":"2022-11-10T16:03:20.9748","exception":false,"start_time":"2022-11-10T16:03:20.97107","status":"completed"},"tags":[]}},{"cell_type":"code","source":"VER = 6\n\nimport pandas as pd, numpy as np\nfrom tqdm.notebook import tqdm\nimport os, sys, pickle, glob, gc\nfrom collections import Counter\nimport cudf, itertools\nprint('We will use RAPIDS version',cudf.__version__)","metadata":{"papermill":{"duration":3.036143,"end_time":"2022-11-10T16:03:24.014816","exception":false,"start_time":"2022-11-10T16:03:20.978673","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-01-01T16:36:00.835909Z","iopub.execute_input":"2023-01-01T16:36:00.836406Z","iopub.status.idle":"2023-01-01T16:36:00.843320Z","shell.execute_reply.started":"2023-01-01T16:36:00.836362Z","shell.execute_reply":"2023-01-01T16:36:00.841539Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"We will use RAPIDS version 21.10.01\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Compute Three Co-visitation Matrices with RAPIDS\nWe will compute 3 co-visitation matrices using RAPIDS cuDF on GPU. This is 30x faster than using Pandas CPU like other public notebooks! For maximum speed, set the variable `DISK_PIECES` to the smallest number possible based on the GPU you are using without incurring memory errors. If you run this code offline with 32GB GPU ram, then you can use `DISK_PIECES = 1` and compute each co-visitation matrix in almost 1 minute! Kaggle's GPU only has 16GB ram, so we use `DISK_PIECES = 4` and it takes an amazing 3 minutes each! Below are some of the tricks to speed up computation\n* Use RAPIDS cuDF GPU instead of Pandas CPU\n* Read disk once and save in CPU RAM for later GPU multiple use\n* Process largest amount of data possible on GPU at one time\n* Merge data in two stages. Multiple small to single medium. Multiple medium to single large.\n* Write result as parquet instead of dictionary","metadata":{"papermill":{"duration":0.00424,"end_time":"2022-11-10T16:03:24.023816","exception":false,"start_time":"2022-11-10T16:03:24.019576","status":"completed"},"tags":[]}},{"cell_type":"code","source":"MODE = \"local\" # \"kaggle\"\n\nif MODE == \"kaggle\":\n    readpath = '../input/otto-chunk-data-inparquet-format/*_parquet/*'\n\nelif MODE == \"local\":\n    readpath = '/kaggle/input/otto-validation/*_parquet/*'\n\n    \nfiles = glob.glob(readpath)","metadata":{"execution":{"iopub.status.busy":"2023-01-01T16:36:00.845159Z","iopub.execute_input":"2023-01-01T16:36:00.846206Z","iopub.status.idle":"2023-01-01T16:36:00.894375Z","shell.execute_reply.started":"2023-01-01T16:36:00.846170Z","shell.execute_reply":"2023-01-01T16:36:00.893512Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# CACHE FUNCTIONS\ndef read_file(f):\n    return cudf.DataFrame( data_cache[f] )\ndef read_file_to_cache(f):\n    df = pd.read_parquet(f)\n    df.ts = (df.ts/1000).astype('int32')\n    df['type'] = df['type'].map(type_labels).astype('int8')\n    return df\n\n# CACHE THE DATA ON CPU BEFORE PROCESSING ON GPU\ndata_cache = {}\ntype_labels = {'clicks':0, 'carts':1, 'orders':2}\nfor f in files: data_cache[f] = read_file_to_cache(f)\n\n# CHUNK PARAMETERS\nREAD_CT = 5\nCHUNK = int( np.ceil( len(files)/6 ))\nprint(f'We will process {len(files)} files, in groups of {READ_CT} and chunks of {CHUNK}.')","metadata":{"papermill":{"duration":0.063943,"end_time":"2022-11-10T16:03:24.091816","exception":false,"start_time":"2022-11-10T16:03:24.027873","status":"completed"},"tags":[],"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-01-01T16:36:00.897228Z","iopub.execute_input":"2023-01-01T16:36:00.897478Z","iopub.status.idle":"2023-01-01T16:36:42.771575Z","shell.execute_reply.started":"2023-01-01T16:36:00.897455Z","shell.execute_reply":"2023-01-01T16:36:42.770455Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"We will process 120 files, in groups of 5 and chunks of 20.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 1) \"Carts Orders\" Co-visitation Matrix - Type Weighted","metadata":{"papermill":{"duration":0.004089,"end_time":"2022-11-10T16:03:24.100502","exception":false,"start_time":"2022-11-10T16:03:24.096413","status":"completed"},"tags":[]}},{"cell_type":"code","source":"type_weight = {0:1, 1:5, 2:4}\n\n# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\nDISK_PIECES = 4\nSIZE = 1.86e6/DISK_PIECES\n\n# COMPUTE IN PARTS FOR MEMORY MANGEMENT\nfor PART in range(DISK_PIECES):\n    print()\n    print('### DISK PART',PART+1)\n    \n    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n    # => OUTER CHUNKS\n    for j in range(6):\n        a = j*CHUNK\n        b = min( (j+1)*CHUNK, len(files) )\n        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n        \n        # => INNER CHUNKS\n        for k in range(a,b,READ_CT):\n            # READ FILE\n            df = [read_file(files[k])]\n            for i in range(1,READ_CT): \n                if k+i<b: df.append( read_file(files[k+i]) )\n            df = cudf.concat(df,ignore_index=True,axis=0)\n            df = df.sort_values(['session','ts'],ascending=[True,False])\n            \n            # USE TAIL OF SESSION\n            df = df.reset_index(drop=True)\n            df['n'] = df.groupby('session').cumcount()\n            df = df.loc[df.n<30].drop('n',axis=1)\n            \n            # CREATE PAIRS\n            df = df.merge(df,on='session')\n            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n            \n            # MEMORY MANAGEMENT COMPUTE IN PARTS\n            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n            \n            # ASSIGN WEIGHTS\n            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y', 'type_y'])\n            df['wgt'] = df.type_y.map(type_weight)\n            df = df[['aid_x','aid_y','wgt']]\n            df.wgt = df.wgt.astype('float32')\n            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n            \n            # COMBINE INNER CHUNKS\n            if k==a: tmp2 = df\n            else: tmp2 = tmp2.add(df, fill_value=0)\n            print(k,', ',end='')\n        \n        print()\n        \n        # COMBINE OUTER CHUNKS\n        if a==0: tmp = tmp2\n        else: tmp = tmp.add(tmp2, fill_value=0)\n        del tmp2, df\n        gc.collect()\n\n    # CONVERT MATRIX TO DICTIONARY\n    tmp = tmp.reset_index()\n    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n    \n    # SAVE TOP 40\n    tmp = tmp.reset_index(drop=True)\n    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n    tmp = tmp.loc[tmp.n<50].drop('n',axis=1)\n    \n    # SAVE PART TO DISK (convert to pandas first uses less memory)\n    tmp.to_pandas().to_parquet(f'{MODE}_top_50_carts_orders_v{VER}_{PART}.pqt')","metadata":{"papermill":{"duration":566.561189,"end_time":"2022-11-10T16:12:50.666123","exception":false,"start_time":"2022-11-10T16:03:24.104934","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-01-01T16:36:42.773311Z","iopub.execute_input":"2023-01-01T16:36:42.773720Z","iopub.status.idle":"2023-01-01T16:38:57.202384Z","shell.execute_reply.started":"2023-01-01T16:36:42.773666Z","shell.execute_reply":"2023-01-01T16:38:57.201386Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"\n### DISK PART 1\nProcessing files 0 thru 19 in groups of 5...\n0 , 5 , 10 , 15 , \nProcessing files 20 thru 39 in groups of 5...\n20 , 25 , 30 , 35 , \nProcessing files 40 thru 59 in groups of 5...\n40 , 45 , 50 , 55 , \nProcessing files 60 thru 79 in groups of 5...\n60 , 65 , 70 , 75 , \nProcessing files 80 thru 99 in groups of 5...\n80 , 85 , 90 , 95 , \nProcessing files 100 thru 119 in groups of 5...\n100 , 105 , 110 , 115 , \n\n### DISK PART 2\nProcessing files 0 thru 19 in groups of 5...\n0 , 5 , 10 , 15 , \nProcessing files 20 thru 39 in groups of 5...\n20 , 25 , 30 , 35 , \nProcessing files 40 thru 59 in groups of 5...\n40 , 45 , 50 , 55 , \nProcessing files 60 thru 79 in groups of 5...\n60 , 65 , 70 , 75 , \nProcessing files 80 thru 99 in groups of 5...\n80 , 85 , 90 , 95 , \nProcessing files 100 thru 119 in groups of 5...\n100 , 105 , 110 , 115 , \n\n### DISK PART 3\nProcessing files 0 thru 19 in groups of 5...\n0 , 5 , 10 , 15 , \nProcessing files 20 thru 39 in groups of 5...\n20 , 25 , 30 , 35 , \nProcessing files 40 thru 59 in groups of 5...\n40 , 45 , 50 , 55 , \nProcessing files 60 thru 79 in groups of 5...\n60 , 65 , 70 , 75 , \nProcessing files 80 thru 99 in groups of 5...\n80 , 85 , 90 , 95 , \nProcessing files 100 thru 119 in groups of 5...\n100 , 105 , 110 , 115 , \n\n### DISK PART 4\nProcessing files 0 thru 19 in groups of 5...\n0 , 5 , 10 , 15 , \nProcessing files 20 thru 39 in groups of 5...\n20 , 25 , 30 , 35 , \nProcessing files 40 thru 59 in groups of 5...\n40 , 45 , 50 , 55 , \nProcessing files 60 thru 79 in groups of 5...\n60 , 65 , 70 , 75 , \nProcessing files 80 thru 99 in groups of 5...\n80 , 85 , 90 , 95 , \nProcessing files 100 thru 119 in groups of 5...\n100 , 105 , 110 , 115 , \n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2) \"Buy2Buy\" Co-visitation Matrix","metadata":{"papermill":{"duration":0.03219,"end_time":"2022-11-10T16:12:50.730634","exception":false,"start_time":"2022-11-10T16:12:50.698444","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\nDISK_PIECES = 1\nSIZE = 1.86e6/DISK_PIECES\n\n# COMPUTE IN PARTS FOR MEMORY MANGEMENT\nfor PART in range(DISK_PIECES):\n    print()\n    print('### DISK PART',PART+1)\n    \n    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n    # => OUTER CHUNKS\n    for j in range(6):\n        a = j*CHUNK\n        b = min( (j+1)*CHUNK, len(files) )\n        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n        \n        # => INNER CHUNKS\n        for k in range(a,b,READ_CT):\n            \n            # READ FILE\n            df = [read_file(files[k])]\n            for i in range(1,READ_CT): \n                if k+i<b: df.append( read_file(files[k+i]) )\n            df = cudf.concat(df,ignore_index=True,axis=0)\n            df = df.loc[df['type'].isin([1,2])] # ONLY WANT CARTS AND ORDERS\n            df = df.sort_values(['session','ts'],ascending=[True,False])\n            \n            # USE TAIL OF SESSION\n            df = df.reset_index(drop=True)\n            df['n'] = df.groupby('session').cumcount()\n            df = df.loc[df.n<30].drop('n',axis=1)\n            \n            # CREATE PAIRS\n            df = df.merge(df,on='session')\n            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y) ] # 14 DAYS\n            \n            # MEMORY MANAGEMENT COMPUTE IN PARTS\n            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n            \n            # ASSIGN WEIGHTS\n            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y', 'type_y'])\n            df['wgt'] = 1\n            df = df[['aid_x','aid_y','wgt']]\n            df.wgt = df.wgt.astype('float32')\n            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n            \n            # COMBINE INNER CHUNKS\n            if k==a: tmp2 = df\n            else: tmp2 = tmp2.add(df, fill_value=0)\n            print(k,', ',end='')\n\n        print()\n        \n        # COMBINE OUTER CHUNKS\n        if a==0: tmp = tmp2\n        else: tmp = tmp.add(tmp2, fill_value=0)\n        del tmp2, df\n        gc.collect()\n\n    # CONVERT MATRIX TO DICTIONARY\n    tmp = tmp.reset_index()\n    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n    \n    # SAVE TOP 40\n    tmp = tmp.reset_index(drop=True)\n    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n    tmp = tmp.loc[tmp.n<50].drop('n',axis=1)\n    \n    # SAVE PART TO DISK (convert to pandas first uses less memory)\n    tmp.to_pandas().to_parquet(f'{MODE}_top_50_buy2buy_v{VER}_{PART}.pqt')","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"papermill":{"duration":113.735315,"end_time":"2022-11-10T16:14:44.498182","exception":false,"start_time":"2022-11-10T16:12:50.762867","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-01-01T16:38:57.204884Z","iopub.execute_input":"2023-01-01T16:38:57.205334Z","iopub.status.idle":"2023-01-01T16:39:17.622466Z","shell.execute_reply.started":"2023-01-01T16:38:57.205298Z","shell.execute_reply":"2023-01-01T16:39:17.621480Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"\n### DISK PART 1\nProcessing files 0 thru 19 in groups of 5...\n0 , 5 , ","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/cudf/core/frame.py:2600: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n  \"When using a sequence of booleans for `ascending`, \"\n","output_type":"stream"},{"name":"stdout","text":"10 , 15 , \nProcessing files 20 thru 39 in groups of 5...\n20 , 25 , 30 , 35 , \nProcessing files 40 thru 59 in groups of 5...\n40 , 45 , 50 , 55 , \nProcessing files 60 thru 79 in groups of 5...\n60 , 65 , 70 , 75 , \nProcessing files 80 thru 99 in groups of 5...\n80 , 85 , 90 , 95 , \nProcessing files 100 thru 119 in groups of 5...\n100 , 105 , 110 , 115 , \n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 3) \"Clicks\" Co-visitation Matrix - Time Weighted","metadata":{"papermill":{"duration":0.04526,"end_time":"2022-11-10T16:14:44.58589","exception":false,"start_time":"2022-11-10T16:14:44.54063","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\nDISK_PIECES = 4\nSIZE = 1.86e6/DISK_PIECES\n\n# COMPUTE IN PARTS FOR MEMORY MANGEMENT\nfor PART in range(DISK_PIECES):\n    print()\n    print('### DISK PART',PART+1)\n    \n    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n    # => OUTER CHUNKS\n    for j in range(6):\n        a = j*CHUNK\n        b = min( (j+1)*CHUNK, len(files) )\n        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n        \n        # => INNER CHUNKS\n        for k in range(a,b,READ_CT):\n            # READ FILE\n            df = [read_file(files[k])]\n            for i in range(1,READ_CT): \n                if k+i<b: df.append( read_file(files[k+i]) )\n            df = cudf.concat(df,ignore_index=True,axis=0)\n            df = df.sort_values(['session','ts'],ascending=[True,False])\n            \n            # USE TAIL OF SESSION\n            df = df.reset_index(drop=True)\n            df['n'] = df.groupby('session').cumcount()\n            df = df.loc[df.n<30].drop('n',axis=1)\n            \n            # CREATE PAIRS\n            df = df.merge(df,on='session')\n            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n            \n            # MEMORY MANAGEMENT COMPUTE IN PARTS\n            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n            \n            # ASSIGN WEIGHTS\n            df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n            df['wgt'] = 1 + 3*(df.ts_x - 1659304800)/(1662328791-1659304800)\n            # 1659304800 : minimum timestamp\n            # 1662328791 : maximum timestamp\n            df = df[['aid_x','aid_y','wgt']]\n            df.wgt = df.wgt.astype('float32')\n            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n            \n            # COMBINE INNER CHUNKS\n            if k==a: tmp2 = df\n            else: tmp2 = tmp2.add(df, fill_value=0)\n            print(k,', ',end='')\n        print()\n        \n        # COMBINE OUTER CHUNKS\n        if a==0: tmp = tmp2\n        else: tmp = tmp.add(tmp2, fill_value=0)\n        del tmp2, df\n        gc.collect()\n\n    # CONVERT MATRIX TO DICTIONARY\n    tmp = tmp.reset_index()\n    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n    \n    # SAVE TOP 40\n    tmp = tmp.reset_index(drop=True)\n    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n    tmp = tmp.loc[tmp.n<50].drop('n',axis=1)\n    \n    # SAVE PART TO DISK (convert to pandas first uses less memory)\n    tmp.to_pandas().to_parquet(f'{MODE}_top_50_clicks_v{VER}_{PART}.pqt')","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"papermill":{"duration":null,"end_time":null,"exception":false,"start_time":"2022-11-10T16:14:44.629032","status":"running"},"tags":[],"execution":{"iopub.status.busy":"2023-01-01T16:39:17.624059Z","iopub.execute_input":"2023-01-01T16:39:17.624823Z","iopub.status.idle":"2023-01-01T16:41:29.476127Z","shell.execute_reply.started":"2023-01-01T16:39:17.624785Z","shell.execute_reply":"2023-01-01T16:41:29.475104Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"\n### DISK PART 1\nProcessing files 0 thru 19 in groups of 5...\n0 , 5 , 10 , 15 , \nProcessing files 20 thru 39 in groups of 5...\n20 , 25 , 30 , 35 , \nProcessing files 40 thru 59 in groups of 5...\n40 , 45 , 50 , 55 , \nProcessing files 60 thru 79 in groups of 5...\n60 , 65 , 70 , 75 , \nProcessing files 80 thru 99 in groups of 5...\n80 , 85 , 90 , 95 , \nProcessing files 100 thru 119 in groups of 5...\n100 , 105 , 110 , 115 , \n\n### DISK PART 2\nProcessing files 0 thru 19 in groups of 5...\n0 , 5 , 10 , 15 , \nProcessing files 20 thru 39 in groups of 5...\n20 , 25 , 30 , 35 , \nProcessing files 40 thru 59 in groups of 5...\n40 , 45 , 50 , 55 , \nProcessing files 60 thru 79 in groups of 5...\n60 , 65 , 70 , 75 , \nProcessing files 80 thru 99 in groups of 5...\n80 , 85 , 90 , 95 , \nProcessing files 100 thru 119 in groups of 5...\n100 , 105 , 110 , 115 , \n\n### DISK PART 3\nProcessing files 0 thru 19 in groups of 5...\n0 , 5 , 10 , 15 , \nProcessing files 20 thru 39 in groups of 5...\n20 , 25 , 30 , 35 , \nProcessing files 40 thru 59 in groups of 5...\n40 , 45 , 50 , 55 , \nProcessing files 60 thru 79 in groups of 5...\n60 , 65 , 70 , 75 , \nProcessing files 80 thru 99 in groups of 5...\n80 , 85 , 90 , 95 , \nProcessing files 100 thru 119 in groups of 5...\n100 , 105 , 110 , 115 , \n\n### DISK PART 4\nProcessing files 0 thru 19 in groups of 5...\n0 , 5 , 10 , 15 , \nProcessing files 20 thru 39 in groups of 5...\n20 , 25 , 30 , 35 , \nProcessing files 40 thru 59 in groups of 5...\n40 , 45 , 50 , 55 , \nProcessing files 60 thru 79 in groups of 5...\n60 , 65 , 70 , 75 , \nProcessing files 80 thru 99 in groups of 5...\n80 , 85 , 90 , 95 , \nProcessing files 100 thru 119 in groups of 5...\n100 , 105 , 110 , 115 , \n","output_type":"stream"}]},{"cell_type":"code","source":"# FREE MEMORY\ndel data_cache, tmp\n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-01-01T16:41:29.477451Z","iopub.execute_input":"2023-01-01T16:41:29.478215Z","iopub.status.idle":"2023-01-01T16:41:29.629763Z","shell.execute_reply.started":"2023-01-01T16:41:29.478177Z","shell.execute_reply":"2023-01-01T16:41:29.628612Z"},"trusted":true},"execution_count":11,"outputs":[]}]}