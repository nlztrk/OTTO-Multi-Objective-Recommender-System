{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19803c5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T18:49:25.672561Z",
     "iopub.status.busy": "2022-11-28T18:49:25.671835Z",
     "iopub.status.idle": "2022-11-28T18:49:28.346781Z",
     "shell.execute_reply": "2022-11-28T18:49:28.345442Z"
    },
    "papermill": {
     "duration": 2.696152,
     "end_time": "2022-11-28T18:49:28.349900",
     "exception": false,
     "start_time": "2022-11-28T18:49:25.653748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use RAPIDS version 22.10.00a+392.g1558403753\n",
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "VER = 6\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import os, sys, pickle, glob, gc\n",
    "from collections import Counter\n",
    "import cudf, itertools\n",
    "print('We will use RAPIDS version',cudf.__version__)\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(nb_workers=4, progress_bar=True, use_memory_fs=True)\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "from pyarrow.parquet import ParquetFile\n",
    "import pyarrow as pa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da36dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory(df):\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type != object:\n",
    "            cmin = df[col].min()\n",
    "            cmax = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if cmin > np.iinfo(np.int32).min and cmax < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif cmin > np.iinfo(np.int64).min and cmax < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if cmin > np.finfo(np.float32).min and cmax < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2505537",
   "metadata": {
    "papermill": {
     "duration": 0.020472,
     "end_time": "2022-11-28T18:57:43.433117",
     "exception": false,
     "start_time": "2022-11-28T18:57:43.412645",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15a31502",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_FOR = \"kaggle\" # \"kaggle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e70b81b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_labels = {'clicks':0, 'carts':1, 'orders':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df7c530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CANDIDATE_COUNT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf5c77c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISK_PIECES = 4\n",
    "\n",
    "clicks_cov_df = pl.from_pandas(pd.concat([pd.read_parquet(f'../raw_data/{GENERATE_FOR}_covisitation/{GENERATE_FOR}_top_{CANDIDATE_COUNT}_clicks_v{VER}_{k}.pqt') for k in range(0, DISK_PIECES)], ignore_index=True))\n",
    "carts_orders_cov_df = pl.from_pandas(pd.concat([pd.read_parquet(f'../raw_data/{GENERATE_FOR}_covisitation/{GENERATE_FOR}_top_{CANDIDATE_COUNT}_carts_orders_v{VER}_{k}.pqt') for k in range(0, DISK_PIECES)], ignore_index=True))\n",
    "buy2buy_cov_df = pl.from_pandas(pd.concat([pd.read_parquet(f'../raw_data/{GENERATE_FOR}_covisitation/{GENERATE_FOR}_top_{CANDIDATE_COUNT}_buy2buy_v{VER}_{k}.pqt') for k in range(0, 1)], ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9623be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_covisitation_features(input_cand_df,\n",
    "                              input_user_int_df,\n",
    "                              input_covisit_df,\n",
    "                              covisit_name=\"clicks\"):      \n",
    "    \n",
    "    candidates_w_covisit = input_cand_df[[\"session\", \"aid\"]].rename({\"aid\":\"aid_x\"}).\\\n",
    "        join(input_user_int_df.rename({\"aid\":\"aid_y\"})[[\"session\", \"aid_y\"]],\n",
    "              how=\"left\",\n",
    "              on=\"session\").fill_null(0).join(input_covisit_df, how=\"left\", on=[\"aid_x\", \"aid_y\"])#.to_pandas()\n",
    "#     candidates_w_covisit.loc[(candidates_w_covisit.aid_x != candidates_w_covisit.aid_y) &\\\n",
    "#                          (candidates_w_covisit.wgt.isna()), \"wgt\"] = 0\n",
    "\n",
    "\n",
    "    candidates_w_covisit = candidates_w_covisit.fill_null(0.)\n",
    "    \n",
    "    candidates_w_covisit_gby = (\n",
    "        candidates_w_covisit\n",
    "        .groupby([\"session\", \"aid_x\"])\n",
    "        .agg(\n",
    "            [\n",
    "                pl.col('wgt').max().alias(covisit_name + '_covisit_' + \"max\"),\n",
    "                pl.col('wgt').min().alias(covisit_name + '_covisit_' + \"min\"),\n",
    "                pl.col('wgt').std().alias(covisit_name + '_covisit_' + \"std\"),\n",
    "                pl.col('wgt').sum().alias(covisit_name + '_covisit_' + \"sum\"),\n",
    "                pl.col('wgt').mean().alias(covisit_name + '_covisit_' + \"mean\"),\n",
    "                pl.col('wgt').count().alias(covisit_name + '_covisit_' + \"count\"),\n",
    "            ]\n",
    "        )\n",
    "    ).sort(\"session\", reverse=False)\n",
    "\n",
    "#     candidates_w_covisit_gby = candidates_w_covisit.groupby([\"session\", \"aid_x\"]).agg({\"wgt\":[\"max\", \"min\", \"std\",\n",
    "#                                                                                               \"sum\", \"mean\", \"count\"]}).fillna(0)\n",
    "#     candidates_w_covisit_gby.columns = [covisit_name + '_covisit_' +  '_'.join(col) for col in candidates_w_covisit_gby.columns]\n",
    "#     candidates_w_covisit_gby.reset_index(inplace=True)\n",
    "\n",
    "#     candidates_w_covisit_gby = pl.from_pandas(candidates_w_covisit_gby.rename(columns={\"aid_x\":\"aid\"}))\n",
    "    candidates_w_covisit_gby = candidates_w_covisit_gby.rename({\"aid_x\":\"aid\"})\n",
    "    \n",
    "    candidates_w_covisit_gby = candidates_w_covisit_gby.with_column(pl.col(\"aid\").cast(pl.Int32))\n",
    "    candidates_w_covisit_gby = candidates_w_covisit_gby.with_column(pl.col(\"session\").cast(pl.Int32)) \n",
    "    return candidates_w_covisit_gby"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988d59c2",
   "metadata": {},
   "source": [
    "## Generating Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68a2d28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datetime_features(input_df):\n",
    "    input_df[\"datetime\"] = pd.to_datetime(input_df.ts + (2 * 60 * 60), unit='s')\n",
    "    input_df[\"hour\"] = input_df[\"datetime\"].dt.hour\n",
    "    input_df[\"dayofweek\"] = input_df[\"datetime\"].dt.dayofweek\n",
    "    input_df[\"is_weekend\"] = (input_df[\"dayofweek\"]>4).astype(int)\n",
    "    return input_df\n",
    "\n",
    "def datetime_aggregator(input_df,\n",
    "                        group_cols=[],\n",
    "                        wanted_cols=[]):\n",
    "    return_df = input_df.groupby(group_cols).agg(\n",
    "        {'hour':['mean', 'std'],\n",
    "         'dayofweek':['mean', 'std'],\n",
    "         'is_weekend':['mean']\n",
    "        })\n",
    "    return_df.columns = ['_'.join(group_cols) + '_' +  '_'.join(col) for col in return_df.columns]\n",
    "    return return_df\n",
    "\n",
    "def type_distribution_aggregator(input_df, \n",
    "                                 group_cols=[]):\n",
    "    return_df = input_df.groupby(group_cols)['type'].value_counts(normalize=True)\n",
    "    return_df = return_df.unstack('type')\n",
    "    return_df.columns = ['_'.join(group_cols) + '_type' + str(col) + \"_mean\" for col in return_df.columns]\n",
    "    return return_df\n",
    "\n",
    "def existence_amount_aggregator(input_df,\n",
    "                                 group_cols=[],\n",
    "                                wanted_cols=[]):\n",
    "    \n",
    "    return_df = input_df.groupby(group_cols).agg({col:[\"count\"] for col in wanted_cols})\n",
    "    return_df.columns = ['_'.join(group_cols) + '_' +  '_'.join(col) for col in return_df.columns]\n",
    "    \n",
    "    count_cols = list(return_df.columns)\n",
    "    \n",
    "    for count_col in count_cols:  \n",
    "        return_df[count_col.replace(\"count\", \"existed\")] = (return_df[count_col]>0).astype(int)\n",
    "        return_df[count_col.replace(\"count\", \"existed_multiple\")] = (return_df[count_col]>1).astype(int)\n",
    "#         return_df[count_col.replace(\"count\", \"existed_times\")] = (return_df[count_col]).astype(int)\n",
    "    \n",
    "    return_df = return_df[[col for col in return_df.columns if (\"count\" not in col)]]\n",
    "    \n",
    "    return return_df\n",
    "\n",
    "def nunique_aggregator(input_df,\n",
    "                       group_cols=[],\n",
    "                       wanted_cols=[]):\n",
    "    \n",
    "    return_df = input_df.groupby(group_cols).agg({col:[\"nunique\"] for col in wanted_cols})\n",
    "    return_df.columns = ['_'.join(group_cols) + '_' +  '_'.join(col) for col in return_df.columns]\n",
    "\n",
    "    return return_df\n",
    "\n",
    "def is_last_aid_of_the_session(input_df,\n",
    "                               group_cols=[\"session\", \"aid\"],\n",
    "                               wanted_cols=[]\n",
    "                              ):\n",
    "    \n",
    "    return_df = input_df[group_cols].copy()\n",
    "    return_df[\"is_aid_interacted_last\"] = 0\n",
    "    return_df.loc[return_df.session.shift(-1) != return_df.session, \"is_aid_interacted_last\"] = 1\n",
    "    return_df = return_df.groupby(group_cols).agg({\"is_aid_interacted_last\":[\"max\"]})\n",
    "    return_df.columns = [\"is_aid_interacted_last_in_session\"]\n",
    "    return return_df\n",
    "\n",
    "def session_len(input_df,\n",
    "                group_cols=[\"session\"],\n",
    "                wanted_cols=[],\n",
    "                return_min_max=False\n",
    "               ):\n",
    "    return_df = input_df[group_cols + [\"ts\"]].copy()\n",
    "    return_df = return_df.groupby(group_cols).agg({\"ts\":[\"min\", \"max\"]})\n",
    "    return_df.columns = [\"session_start\", \"session_end\"]\n",
    "    return_df[\"session_len\"] = return_df[\"session_end\"] - return_df[\"session_start\"]\n",
    "    \n",
    "    if return_min_max:\n",
    "        return return_df\n",
    "    else:\n",
    "        return return_df[[\"session_len\"]]\n",
    "\n",
    "def aid_session_ts_offsets(input_df,\n",
    "                group_cols=[\"session\", \"aid\"],\n",
    "                wanted_cols=[]):\n",
    "    session_lens = session_len(input_df,\n",
    "                               return_min_max=True).reset_index()\n",
    "    return_df = input_df[group_cols + [\"ts\"]].copy()\n",
    "    return_df = return_df.groupby(group_cols).agg({\"ts\":[\"last\"]})\n",
    "    return_df.columns = [\"session_aid_last_ts\"]\n",
    "    return_df.reset_index(inplace=True)\n",
    "    return_df = return_df.merge(session_lens, how=\"left\", on=\"session\")\n",
    "    return_df[\"aid_ts_session_end_offset\"] = return_df[\"session_end\"] - return_df[\"session_aid_last_ts\"]\n",
    "    return_df[\"aid_ts_session_start_offset\"] = return_df[\"session_aid_last_ts\"] - return_df[\"session_start\"]\n",
    "\n",
    "    return_df = return_df[group_cols + [\"aid_ts_session_end_offset\", \"aid_ts_session_start_offset\"]].set_index(group_cols)\n",
    "    return return_df\n",
    "\n",
    "def type_based_aggregator(input_df,\n",
    "                          group_cols=[],\n",
    "                          wanted_cols=[],\n",
    "                          aggregators=[]):\n",
    "    type_dfs = []\n",
    "    for type_id in range(3):\n",
    "        for aggregator in aggregators:\n",
    "            aggregator_df = aggregator(input_df[input_df.type==type_id].reset_index(drop=True),\n",
    "                                       group_cols=group_cols,\n",
    "                                       wanted_cols=wanted_cols)\n",
    "            aggregator_df.columns = [\"type\" + str(type_id) + \"_\" + col for col in aggregator_df.columns]\n",
    "        type_dfs.append(aggregator_df)\n",
    "        \n",
    "    return pd.concat(type_dfs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ede33468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is read!\n",
      "Item features are created!\n",
      "User features are created!\n",
      "User-Item Interaction features are created!\n"
     ]
    }
   ],
   "source": [
    "if GENERATE_FOR == \"local\":\n",
    "    train_df = pd.read_parquet(f\"./splitted_raw_data/train.parquet\")\n",
    "    val_df = pd.read_parquet(f\"./splitted_raw_data/val.parquet\")\n",
    "\n",
    "elif GENERATE_FOR == \"kaggle\":\n",
    "    train_df = pd.read_parquet(f\"./splitted_raw_data/all_train.parquet\")\n",
    "    val_df = pd.read_parquet(f\"./splitted_raw_data/test.parquet\")\n",
    "\n",
    "train_df = generate_datetime_features(train_df)\n",
    "val_df = generate_datetime_features(val_df)\n",
    "\n",
    "item_df = pd.concat([train_df,val_df], ignore_index=True)\n",
    "user_df = val_df\n",
    "user_item_int_df = val_df    \n",
    "\n",
    "print(\"Data is read!\")\n",
    "\n",
    "############\n",
    "\n",
    "# item_features = item_df.groupby('aid').agg({'aid':['count'], 'session':['nunique']})\n",
    "# item_features.columns = ['aid_' + \"_\".join(col) for col in item_features.columns]\n",
    "\n",
    "item_features = pd.concat([\n",
    "#     item_features,\n",
    "    existence_amount_aggregator(item_df,\n",
    "                                group_cols=[\"aid\"],\n",
    "                                wanted_cols=[\"session\", \"aid\"]),\n",
    "    nunique_aggregator(item_df,\n",
    "                       group_cols=[\"aid\"],\n",
    "                       wanted_cols=[\"session\"]),\n",
    "    datetime_aggregator(item_df,\n",
    "                        group_cols=[\"aid\"]),\n",
    "    type_distribution_aggregator(item_df,\n",
    "                                 group_cols=[\"aid\"]),\n",
    "#     type_based_aggregator(item_df,\n",
    "#                           group_cols=[\"aid\"],\n",
    "#                           wanted_cols=[\"aid\", \"session\"],\n",
    "#                           aggregators=[datetime_aggregator,\n",
    "#                                        nunique_aggregator,\n",
    "#                                        existence_amount_aggregator])\n",
    "], axis=1)\n",
    "\n",
    "item_features = reduce_memory(item_features)\n",
    "\n",
    "item_features.to_parquet(f'./all_features/{GENERATE_FOR}_item_features.pqt')\n",
    "\n",
    "print(\"Item features are created!\")\n",
    "\n",
    "############\n",
    "\n",
    "# user_features = user_df.groupby('session').agg({'session':['count'], 'aid':['nunique']})\n",
    "\n",
    "# user_features.columns = ['session_' + \"_\".join(col) for col in user_features.columns]\n",
    "\n",
    "user_features = pd.concat([\n",
    "#     user_features,\n",
    "    existence_amount_aggregator(user_df,\n",
    "                                group_cols=[\"session\"],\n",
    "                                wanted_cols=[\"session\", \"aid\"]),\n",
    "    session_len(user_df),\n",
    "#     nunique_aggregator(user_df,\n",
    "#                        group_cols=[\"session\"],\n",
    "#                        wanted_cols=[\"aid\"]),\n",
    "    datetime_aggregator(user_df,\n",
    "                        group_cols=[\"session\"]),\n",
    "    type_distribution_aggregator(user_df,\n",
    "                                 group_cols=[\"session\"]),\n",
    "#     type_based_aggregator(user_df,\n",
    "#                           group_cols=[\"session\"],\n",
    "#                           wanted_cols=[\"aid\", \"session\"],\n",
    "#                           aggregators=[datetime_aggregator,\n",
    "#                                        session_len,\n",
    "# #                                        nunique_aggregator,\n",
    "#                                        existence_amount_aggregator])\n",
    "], axis=1)\n",
    "\n",
    "user_features = reduce_memory(user_features)\n",
    "\n",
    "user_features.to_parquet(f'./all_features/{GENERATE_FOR}_user_features.pqt')\n",
    "\n",
    "print(\"User features are created!\")\n",
    "\n",
    "############\n",
    "\n",
    "# user_item_int_features = user_item_int_df.groupby(['session', 'aid']).agg({'aid':['count']})\n",
    "\n",
    "# user_item_int_features.columns = ['session_aid_' + \"_\".join(col) for col in user_item_int_features.columns]\n",
    "\n",
    "user_item_int_features = pd.concat([\n",
    "#     user_item_int_features,\n",
    "    existence_amount_aggregator(user_item_int_df,\n",
    "                                group_cols=[\"session\", \"aid\"],\n",
    "                                wanted_cols=[\"aid\"]),\n",
    "    is_last_aid_of_the_session(user_item_int_df),\n",
    "    aid_session_ts_offsets(user_item_int_df),\n",
    "#     nunique_aggregator(user_df,\n",
    "#                        group_cols=[\"session\"],\n",
    "#                        wanted_cols=[\"aid\"]),\n",
    "    datetime_aggregator(user_item_int_df,\n",
    "                        group_cols=['session', 'aid']),\n",
    "    type_distribution_aggregator(user_item_int_df,\n",
    "                                 group_cols=['session', 'aid']),\n",
    "#     type_based_aggregator(user_item_int_df,\n",
    "#                           group_cols=['session', 'aid'],\n",
    "#                           wanted_cols=[\"aid\"],\n",
    "#                           aggregators=[datetime_aggregator,\n",
    "#                                        is_last_aid_of_the_session,\n",
    "#                                        aid_session_ts_offsets,\n",
    "# #                                        nunique_aggregator,\n",
    "#                                        existence_amount_aggregator])\n",
    "], axis=1)\n",
    "\n",
    "user_item_int_features = reduce_memory(user_item_int_features)\n",
    "\n",
    "user_item_int_features.to_parquet(f'./all_features/{GENERATE_FOR}_user_item_int_features.pqt')\n",
    "\n",
    "print(\"User-Item Interaction features are created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a360283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del item_features, user_features, user_item_int_features\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a850637",
   "metadata": {},
   "outputs": [],
   "source": [
    "del item_df, train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03248e26",
   "metadata": {},
   "source": [
    "## Merging Features w/ Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36ce29ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "021790881e8d41dc8d53c4a091bbeed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb4cd6615fd470aa607eccbbe3cb399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7b88d622594706adc8b0a58c06ff67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ebfce500e724c2eaae10e5a20909579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "item_features = pl.read_parquet(f'./all_features/{GENERATE_FOR}_item_features.pqt')\n",
    "user_features = pl.read_parquet(f'./all_features/{GENERATE_FOR}_user_features.pqt')\n",
    "user_item_int_features = pl.read_parquet(f'./all_features/{GENERATE_FOR}_user_item_int_features.pqt')\n",
    "\n",
    "if GENERATE_FOR == \"local\":\n",
    "    val_df = pl.read_parquet(f\"./splitted_raw_data/val.parquet\")\n",
    "elif GENERATE_FOR == \"kaggle\":\n",
    "    val_df = pl.read_parquet(f\"./splitted_raw_data/test.parquet\")\n",
    "    \n",
    "for type_str in tqdm(list(type_labels.keys())):\n",
    "    \n",
    "    pf = ParquetFile(f\"./candidate_data/{GENERATE_FOR}_{CANDIDATE_COUNT}candidates_{type_str}.parquet\")\n",
    "    chunk = 10_000_000\n",
    "    \n",
    "    total_candidate_df = 0\n",
    "    \n",
    "    \n",
    "    for batch_i, batch in tqdm(enumerate(pf.iter_batches(batch_size = chunk))):\n",
    "        candidate_df = batch.to_pandas()\n",
    "        candidate_df = pl.from_pandas(candidate_df)\n",
    "        \n",
    "        candidate_df = candidate_df.with_column(pl.col(\"aid\").cast(pl.Int32))\n",
    "        candidate_df = candidate_df.with_column(pl.col(\"session\").cast(pl.Int32)) \n",
    "        \n",
    "        val_df = val_df.with_column(pl.col(\"aid\").cast(pl.Int32))\n",
    "        val_df = val_df.with_column(pl.col(\"session\").cast(pl.Int32))\n",
    "        \n",
    "        for covisit in[[clicks_cov_df, \"clicks\"],\n",
    "                       [carts_orders_cov_df, \"carts_orders\"],\n",
    "                       [buy2buy_cov_df, \"buy2buy\"]]:\n",
    "            \n",
    "            covisit[0] = covisit[0].with_column(pl.col(\"aid_x\").cast(pl.Int32))\n",
    "            covisit[0] = covisit[0].with_column(pl.col(\"aid_y\").cast(pl.Int32))\n",
    "        \n",
    "            candidate_df = candidate_df.join(\n",
    "                get_covisitation_features(input_cand_df=candidate_df,\n",
    "                                          input_user_int_df=val_df,\n",
    "                                          input_covisit_df=covisit[0],\n",
    "                                          covisit_name=covisit[1]),\n",
    "                how=\"left\",\n",
    "                on=[\"session\", \"aid\"])\n",
    "                \n",
    "        candidate_df = candidate_df.with_column(pl.col(\"aid\").cast(pl.Int64))\n",
    "        candidate_df = candidate_df.with_column(pl.col(\"session\").cast(pl.Int64))         \n",
    "        \n",
    "        #candidate_df = pl.read_parquet(f\"./candidate_data/{GENERATE_FOR}_candidates_{type_str}.parquet\").drop(\"__index_level_0__\")\n",
    "        rank_repeater = np.hstack([list(range(1,CANDIDATE_COUNT+1)) for i in range(int(len(candidate_df)/CANDIDATE_COUNT))])\n",
    "        candidate_df = candidate_df.with_column(pl.Series(name=\"candidate_rank\", values=rank_repeater))\n",
    "        del rank_repeater;gc.collect()\n",
    "        #print('Candidate Rank Features, Done...')\n",
    "        candidate_df = candidate_df.join(item_features, on='aid', how='left').fill_null(-1)\n",
    "        #print('Item Features, Done...')\n",
    "        candidate_df = candidate_df.join(user_features, on='session', how='left').fill_null(-1)\n",
    "        #print('User Features, Done...')\n",
    "        candidate_df = candidate_df.join(user_item_int_features,\n",
    "                                          on=['session', 'aid'],\n",
    "                                          how='left').fill_null(-1)\n",
    "        #print('User-Item Features, Done...')\n",
    "        tar = pd.read_parquet('./splitted_raw_data/val_labels.parquet')\n",
    "        tar = tar.loc[ tar['type'] == type_str ]\n",
    "        aids = tar.ground_truth.explode().rename('aid')\n",
    "        tar = tar[['session']]\n",
    "        tar = tar.merge(aids, left_index=True, right_index=True, how='left')\n",
    "        tar['label'] = 1\n",
    "        #print('Extract Labels, Done...')\n",
    "        \n",
    "        tar = pl.from_pandas(tar)\n",
    "        \n",
    "        candidate_df = candidate_df.join(tar, on=['session','aid'], how='left').fill_null(0)\n",
    "        \n",
    "        candidate_df.write_parquet(f'./candidated_features/{GENERATE_FOR}_{type_str}_all_data_{CANDIDATE_COUNT}candidates_p{batch_i}.pqt')\n",
    "        \n",
    "        del candidate_df,tar,aids;gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b4d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dff = pd.read_parquet(f'./candidated_features/{GENERATE_FOR}_clicks_all_data.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d7ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47034497",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436c2480",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_covisitation_features(input_cand_df=candidate_df,\n",
    "                                          input_user_int_df=val_df,\n",
    "                                          input_covisit_df=covisit[0],\n",
    "                                          covisit_name=covisit[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc4ccca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3695.647257,
   "end_time": "2022-11-28T19:50:53.428271",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-28T18:49:17.781014",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
