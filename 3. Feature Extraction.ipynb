{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19803c5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T18:49:25.672561Z",
     "iopub.status.busy": "2022-11-28T18:49:25.671835Z",
     "iopub.status.idle": "2022-11-28T18:49:28.346781Z",
     "shell.execute_reply": "2022-11-28T18:49:28.345442Z"
    },
    "papermill": {
     "duration": 2.696152,
     "end_time": "2022-11-28T18:49:28.349900",
     "exception": false,
     "start_time": "2022-11-28T18:49:25.653748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "VER = 6\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import os, sys, pickle, glob, gc\n",
    "from collections import Counter\n",
    "import cudf, itertools\n",
    "print('We will use RAPIDS version',cudf.__version__)\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(nb_workers=4, progress_bar=True, use_memory_fs=True)\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "from pyarrow.parquet import ParquetFile\n",
    "import pyarrow as pa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da36dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory(df):\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type != object:\n",
    "            cmin = df[col].min()\n",
    "            cmax = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if cmin > np.iinfo(np.int32).min and cmax < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif cmin > np.iinfo(np.int64).min and cmax < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if cmin > np.finfo(np.float32).min and cmax < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2505537",
   "metadata": {
    "papermill": {
     "duration": 0.020472,
     "end_time": "2022-11-28T18:57:43.433117",
     "exception": false,
     "start_time": "2022-11-28T18:57:43.412645",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a31502",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_FOR = \"kaggle\" # \"kaggle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b81b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_labels = {'clicks':0, 'carts':1, 'orders':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7c530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CANDIDATE_COUNT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfedd66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_FOR == \"local\":\n",
    "    train_path = \"./splitted_raw_data/train.parquet\"\n",
    "    val_path = \"./splitted_raw_data/val.parquet\"\n",
    "    \n",
    "elif GENERATE_FOR == \"kaggle\":\n",
    "    train_path = \"./splitted_raw_data/all_train.parquet\"\n",
    "    val_path = \"./splitted_raw_data/test.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d255fb05",
   "metadata": {},
   "source": [
    "### CANDIDATE COVISIT FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119fee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_covisitation_features(input_cand_df,\n",
    "                              input_user_int_df,\n",
    "                              input_covisit_df,\n",
    "                              covisit_name=\"clicks\",\n",
    "                              score_col=\"wgt\",\n",
    "                              scoring_name=\"covisitation\",\n",
    "                              session_history_wanted_types=None):      \n",
    "    \n",
    "    if session_history_wanted_types:\n",
    "        filtered_input_user_int_df = input_user_int_df.filter(pl.col('type').is_in(session_history_wanted_types))\n",
    "    else: \n",
    "        filtered_input_user_int_df = input_user_int_df\n",
    "    \n",
    "    candidates_w_covisit = input_cand_df[[\"session\", \"aid\"]].rename({\"aid\":\"aid_x\"}).\\\n",
    "    join(filtered_input_user_int_df.rename({\"aid\":\"aid_y\"})[[\"session\", \"aid_y\"]],\n",
    "         how=\"left\",\n",
    "         on=\"session\").fill_null(0).join(input_covisit_df, how=\"left\", on=[\"aid_x\", \"aid_y\"])\n",
    "\n",
    "    candidates_w_covisit = candidates_w_covisit.fill_null(0.)\n",
    "    \n",
    "    candidates_w_covisit_gby = (\n",
    "        candidates_w_covisit\n",
    "        .groupby([\"session\", \"aid_x\"])\n",
    "        .agg(\n",
    "            [\n",
    "                pl.col(score_col).max().alias(covisit_name + '_' + scoring_name + '_' + \"max\"),\n",
    "                pl.col(score_col).min().alias(covisit_name + '_' + scoring_name + '_' + \"min\"),\n",
    "                pl.col(score_col).std().alias(covisit_name + '_' + scoring_name + '_' + \"std\"),\n",
    "                pl.col(score_col).sum().alias(covisit_name + '_' + scoring_name + '_' + \"sum\"),\n",
    "                pl.col(score_col).mean().alias(covisit_name + '_' + scoring_name + '_' + \"mean\"),\n",
    "                pl.col(score_col).count().alias(covisit_name + '_' + scoring_name + '_' + \"count\"),\n",
    "            ]\n",
    "        )\n",
    "    ).sort(\"session\", reverse=False)\n",
    "\n",
    "    candidates_w_covisit_gby = candidates_w_covisit_gby.rename({\"aid_x\":\"aid\"})\n",
    "    \n",
    "    candidates_w_covisit_gby = candidates_w_covisit_gby.with_column(pl.col(\"aid\").cast(pl.Int32))\n",
    "    candidates_w_covisit_gby = candidates_w_covisit_gby.with_column(pl.col(\"session\").cast(pl.Int32)) \n",
    "    return candidates_w_covisit_gby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977affcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidate_history_pair_score_features(input_val_df_path,\n",
    "                                                   score_df_tuples_w_names,\n",
    "                                                   score_col,\n",
    "                                                   scoring_name=\"covisitation\"):\n",
    "    \n",
    "    val_df = pl.read_parquet(input_val_df_path)\n",
    "\n",
    "    for type_str in tqdm(list(type_labels.keys())):\n",
    "\n",
    "        pf = ParquetFile(f\"./candidate_data/{GENERATE_FOR}_{CANDIDATE_COUNT}candidates_{type_str}.parquet\")\n",
    "        chunk = 10_000_000\n",
    "\n",
    "        total_candidate_df = []\n",
    "\n",
    "        for batch_i, batch in tqdm(enumerate(pf.iter_batches(batch_size = chunk))):\n",
    "            candidate_df = batch.to_pandas()\n",
    "            del batch\n",
    "            candidate_df = pl.from_pandas(candidate_df)\n",
    "\n",
    "            candidate_df = candidate_df.with_column(pl.col(\"aid\").cast(pl.Int32))\n",
    "            candidate_df = candidate_df.with_column(pl.col(\"session\").cast(pl.Int32)) \n",
    "\n",
    "            val_df = val_df.with_column(pl.col(\"aid\").cast(pl.Int32))\n",
    "            val_df = val_df.with_column(pl.col(\"session\").cast(pl.Int32))\n",
    "\n",
    "            for covisit in score_df_tuples_w_names:\n",
    "                covisit[0] = covisit[0].with_column(pl.col(\"aid_x\").cast(pl.Int32))\n",
    "                covisit[0] = covisit[0].with_column(pl.col(\"aid_y\").cast(pl.Int32))\n",
    "\n",
    "                candidate_df = candidate_df.join(\n",
    "                    get_covisitation_features(input_cand_df=candidate_df,\n",
    "                                              input_user_int_df=val_df,\n",
    "                                              input_covisit_df=covisit[0],\n",
    "                                              covisit_name=covisit[1],\n",
    "                                              score_col=score_col,\n",
    "                                              scoring_name=scoring_name,\n",
    "                                              session_history_wanted_types=covisit[2]),\n",
    "                    how=\"left\",\n",
    "                    on=[\"session\", \"aid\"])\n",
    "\n",
    "            candidate_df = candidate_df.with_column(pl.col(\"aid\").cast(pl.Int64))\n",
    "            candidate_df = candidate_df.with_column(pl.col(\"session\").cast(pl.Int64))         \n",
    "\n",
    "            total_candidate_df.append(candidate_df)\n",
    "\n",
    "            del candidate_df\n",
    "\n",
    "        total_candidate_df = pl.concat(total_candidate_df)    \n",
    "        \n",
    "        total_candidate_df = total_candidate_df.with_columns([pl.col(total_candidate_df.columns[2:]).cast(pl.Float32),])\n",
    "        total_candidate_df.write_parquet(f'../raw_data/{GENERATE_FOR}_{scoring_name}_features/{scoring_name}_features_{type_str}_{CANDIDATE_COUNT}candidates.pqt')\n",
    "\n",
    "        del total_candidate_df;gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b24d90",
   "metadata": {},
   "source": [
    "### Covisitation pair 'wgt' features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fdc52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISK_PIECES = 4\n",
    "\n",
    "print(\"Reading clicks covisitation...\")\n",
    "clicks_cov_df = pl.from_pandas(pd.concat([pd.read_parquet(f'../raw_data/{GENERATE_FOR}_covisitation/{GENERATE_FOR}_top_{CANDIDATE_COUNT}_clicks_v{VER}_{k}.pqt') for k in range(0, DISK_PIECES)], ignore_index=True))\n",
    "print(\"Reading carts-orders covisitation...\")\n",
    "carts_orders_cov_df = pl.from_pandas(pd.concat([pd.read_parquet(f'../raw_data/{GENERATE_FOR}_covisitation/{GENERATE_FOR}_top_{CANDIDATE_COUNT}_carts_orders_v{VER}_{k}.pqt') for k in range(0, DISK_PIECES)], ignore_index=True))\n",
    "print(\"Reading buy2buy covisitation...\")\n",
    "buy2buy_cov_df = pl.from_pandas(pd.concat([pd.read_parquet(f'../raw_data/{GENERATE_FOR}_covisitation/{GENERATE_FOR}_top_{CANDIDATE_COUNT}_buy2buy_v{VER}_{k}.pqt') for k in range(0, 1)], ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda6f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_covisits_features(input_covisit_df,\n",
    "                         score_col,\n",
    "                         covisit_name):\n",
    "    aid_cov_feat_df = (\n",
    "        clicks_cov_df\n",
    "        .groupby([\"aid_x\"])\n",
    "        .agg(\n",
    "            [\n",
    "                pl.col(score_col).max().alias(covisit_name + '_' + score_col + '_' + \"max\"),\n",
    "                pl.col(score_col).min().alias(covisit_name + '_' + score_col + '_' + \"min\"),\n",
    "                pl.col(score_col).std().alias(covisit_name + '_' + score_col + '_' + \"std\"),\n",
    "                pl.col(score_col).sum().alias(covisit_name + '_' + score_col + '_' + \"sum\"),\n",
    "                pl.col(score_col).mean().alias(covisit_name + '_' + score_col + '_' + \"mean\"),\n",
    "                pl.col(score_col).count().alias(covisit_name + '_' + score_col + '_' + \"count\"),\n",
    "            ]\n",
    "        )\n",
    "    ).sort(\"aid_x\", reverse=False).rename({\"aid_x\":\"aid\"})\n",
    "    \n",
    "    aid_cov_feat_df = aid_cov_feat_df.with_columns([pl.col([\"aid\"]).cast(pl.Int64)])\n",
    "\n",
    "    aid_cov_feat_df.write_parquet(f'../raw_data/{GENERATE_FOR}_covisitation_features/{covisit_name}_covisitation_features.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d205bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df_tuples_w_names = [[clicks_cov_df, \"all_clicks\"],\n",
    "                           [carts_orders_cov_df, \"all_carts_orders\"],\n",
    "                           [buy2buy_cov_df, \"all_buy2buy\"]]\n",
    "\n",
    "for covisit_tuple in tqdm(score_df_tuples_w_names):\n",
    "    all_covisits_features(input_covisit_df=covisit_tuple[0],\n",
    "                          score_col=\"wgt\",\n",
    "                          covisit_name=covisit_tuple[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d15db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df_tuples_w_names = [[clicks_cov_df, \"clicks\", None],\n",
    "                           [carts_orders_cov_df, \"carts_orders\", None],\n",
    "                           [buy2buy_cov_df, \"buy2buy\", None]]\n",
    "\n",
    "generate_candidate_history_pair_score_features(input_val_df_path=val_path,\n",
    "                                               score_df_tuples_w_names=score_df_tuples_w_names,\n",
    "                                               score_col=\"wgt\",\n",
    "                                               scoring_name=\"covisitation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baf8c84",
   "metadata": {},
   "source": [
    "### Word2Vec pair 'similarity' features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b8e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading clicks w2v...\")\n",
    "clicks_cov_df = pl.scan_parquet(f'./all_features/{GENERATE_FOR}_top_{CANDIDATE_COUNT}_clicks_w2v_similarities.pqt')\n",
    "clicks_cov_df = clicks_cov_df.rename({\"similarity\":\"w2v_similarity\"})\n",
    "print(\"Reading carts-orders w2v...\")\n",
    "carts_orders_cov_df = pl.scan_parquet(f'./all_features/{GENERATE_FOR}_top_{CANDIDATE_COUNT}_carts_w2v_similarities.pqt')\n",
    "carts_orders_cov_df = carts_orders_cov_df.rename({\"similarity\":\"w2v_similarity\"})\n",
    "print(\"Reading buy2buy w2v...\")\n",
    "buy2buy_cov_df = pl.scan_parquet(f'./all_features/{GENERATE_FOR}_top_{CANDIDATE_COUNT}_buy2buy_w2v_similarities.pqt')\n",
    "buy2buy_cov_df = buy2buy_cov_df.rename({\"similarity\":\"w2v_similarity\"})\n",
    "\n",
    "score_df_tuples_w_names = [[clicks_cov_df, \"clicks\", None],\n",
    "                           [carts_orders_cov_df, \"carts_orders\", None],\n",
    "                           [buy2buy_cov_df, \"buy2buy\", None]]\n",
    "\n",
    "generate_candidate_history_pair_score_features(input_val_df_path=val_path,\n",
    "                                               score_df_tuples_w_names=score_df_tuples_w_names,\n",
    "                                               score_col=\"w2v_similarity\",\n",
    "                                               scoring_name=\"word2vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988d59c2",
   "metadata": {},
   "source": [
    "## Generating Feature Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa48ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datetime_features(input_df):\n",
    "    input_df[\"datetime\"] = pd.to_datetime(input_df.ts + (2 * 60 * 60), unit='s')\n",
    "    input_df[\"hour\"] = input_df[\"datetime\"].dt.hour\n",
    "    input_df[\"dayofweek\"] = input_df[\"datetime\"].dt.dayofweek\n",
    "    input_df[\"is_weekend\"] = (input_df[\"dayofweek\"]>4).astype(int)\n",
    "    return input_df\n",
    "\n",
    "train_df = pd.read_parquet(train_path)\n",
    "val_df = pd.read_parquet(val_path)\n",
    "\n",
    "train_df = generate_datetime_features(train_df)\n",
    "val_df = generate_datetime_features(val_df)\n",
    "\n",
    "item_df = pd.concat([train_df,val_df], ignore_index=True)\n",
    "user_df = val_df\n",
    "user_item_int_df = val_df    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d47b300",
   "metadata": {},
   "source": [
    "### Extract AID-Type Occurence Counts & Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98010e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "aid_count_df = item_df[item_df.type==0].groupby(\"aid\")[\"ts\"].count().rename(\"click_count\").to_frame().reset_index()\n",
    "aid_cart_df = item_df[item_df.type==1].groupby(\"aid\")[\"ts\"].count().rename(\"cart_count\").to_frame().reset_index()\n",
    "aid_order_df = item_df[item_df.type==2].groupby(\"aid\")[\"ts\"].count().rename(\"order_count\").to_frame().reset_index()\n",
    "\n",
    "aid_count_df = aid_count_df.merge(aid_cart_df, how=\"left\", on=\"aid\")\n",
    "aid_count_df = aid_count_df.merge(aid_order_df, how=\"left\", on=\"aid\")\n",
    "\n",
    "aid_count_df.fillna(0., inplace=True)\n",
    "\n",
    "aid_count_df[\"click_prob\"] = aid_count_df[\"click_count\"] / aid_count_df[\"click_count\"].sum()\n",
    "aid_count_df[\"cart_prob\"] = aid_count_df[\"cart_count\"] / aid_count_df[\"cart_count\"].sum()\n",
    "aid_count_df[\"order_prob\"] = aid_count_df[\"order_count\"] / aid_count_df[\"order_count\"].sum()\n",
    "\n",
    "aid_count_df.to_parquet(f'./all_features/{GENERATE_FOR}_aid_occurences.pqt')\n",
    "\n",
    "display(aid_count_df.head())\n",
    "\n",
    "del aid_count_df, aid_order_df, aid_cart_df; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826d96f6",
   "metadata": {},
   "source": [
    "### Co-Occurence Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d38760",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_order_v_order = sorted(glob.glob(f\"../raw_data/{GENERATE_FOR}_cooccurence/{GENERATE_FOR}_(orders)vs(orders)_cooccurences_v10_*.pqt\"))\n",
    "co_order_v_order = pd.concat([pd.read_parquet(f) for f in co_order_v_order], ignore_index=True)\n",
    "\n",
    "co_cart_v_cart = sorted(glob.glob(f\"../raw_data/{GENERATE_FOR}_cooccurence/{GENERATE_FOR}_(carts)vs(carts)_cooccurences_v10_*.pqt\"))\n",
    "co_cart_v_cart = pd.concat([pd.read_parquet(f) for f in co_cart_v_cart], ignore_index=True)\n",
    "\n",
    "co_click_v_order = sorted(glob.glob(f\"../raw_data/{GENERATE_FOR}_cooccurence/{GENERATE_FOR}_(clicks)vs(orders)_cooccurences_v10_*.pqt\"))\n",
    "co_click_v_order = pd.concat([pd.read_parquet(f) for f in co_click_v_order], ignore_index=True)\n",
    "\n",
    "co_cart_v_order = sorted(glob.glob(f\"../raw_data/{GENERATE_FOR}_cooccurence/{GENERATE_FOR}_(carts)vs(orders)_cooccurences_v10_*.pqt\"))\n",
    "co_cart_v_order = pd.concat([pd.read_parquet(f) for f in co_cart_v_order], ignore_index=True)\n",
    "\n",
    "co_click_v_cart = sorted(glob.glob(f\"../raw_data/{GENERATE_FOR}_cooccurence/{GENERATE_FOR}_(clicks)vs(carts)_cooccurences_v10_*.pqt\"))\n",
    "co_click_v_cart = pd.concat([pd.read_parquet(f) for f in co_click_v_cart], ignore_index=True)\n",
    "\n",
    "co_order_v_cart = sorted(glob.glob(f\"../raw_data/{GENERATE_FOR}_cooccurence/{GENERATE_FOR}_(orders)vs(carts)_cooccurences_v10_*.pqt\"))\n",
    "co_order_v_cart = pd.concat([pd.read_parquet(f) for f in co_order_v_cart], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f098ab99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_PPMI_features(co_df,\n",
    "                           count_df,\n",
    "                           aidx_type=\"order\",\n",
    "                           aidy_type=\"order\"):\n",
    "    co_df_w_probs = co_df.\\\n",
    "        merge(count_df[[\"aid\", f\"{aidx_type}_prob\"]], left_on=\"aid_x\", right_on=\"aid\").drop(\"aid\", 1).\\\n",
    "                rename(columns={f\"{aidx_type}_prob\": \"aid_x_prob\"}).\\\n",
    "        merge(count_df[[\"aid\", f\"{aidy_type}_prob\"]], left_on=\"aid_y\", right_on=\"aid\").drop(\"aid\", 1).\\\n",
    "                rename(columns={f\"{aidy_type}_prob\": \"aid_y_prob\"})\n",
    "\n",
    "    co_df_w_probs[\"pair_prob\"] = (co_df_w_probs[\"wgt\"] / co_df_w_probs[\"wgt\"].sum())\n",
    "    \n",
    "    co_df_w_probs[\"PPMI\"] =\\\n",
    "    np.maximum(np.log2(co_df_w_probs[\"pair_prob\"] /\\\n",
    "                       (co_df_w_probs[\"aid_x_prob\"] * co_df_w_probs[\"aid_y_prob\"])), 0)\n",
    "    \n",
    "    return co_df_w_probs[[\"aid_x\", \"aid_y\", \"PPMI\"]]\n",
    "\n",
    "aid_count_df = pd.read_parquet(f'./all_features/{GENERATE_FOR}_aid_occurences.pqt')\n",
    "\n",
    "ppmi_target_combs = [\n",
    "    [co_order_v_order, \"order\", \"order\"],\n",
    "    [co_cart_v_cart, \"cart\", \"cart\"],\n",
    "    [co_click_v_order, \"click\", \"order\"],\n",
    "    [co_cart_v_order, \"cart\", \"order\"],\n",
    "    [co_click_v_cart, \"click\", \"cart\"],\n",
    "    [co_order_v_cart, \"order\", \"cart\"],\n",
    "]\n",
    "\n",
    "for ppmi_target_comb in ppmi_target_combs:\n",
    "    ppmi_df = generate_PPMI_features(co_df=ppmi_target_comb[0],\n",
    "                                     count_df=aid_count_df,\n",
    "                                     aidx_type=ppmi_target_comb[1],\n",
    "                                     aidy_type=ppmi_target_comb[2])\n",
    "    \n",
    "    ppmi_df.to_parquet(f'./all_features/{GENERATE_FOR}_ppmi_{ppmi_target_comb[1]}_{ppmi_target_comb[2]}.pqt')\n",
    "    display(ppmi_df.head())\n",
    "    del ppmi_df; gc.collect()\n",
    "    \n",
    "del co_order_v_order, co_cart_v_cart, co_click_v_order, co_cart_v_order, co_click_v_cart, co_order_v_cart; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff2ff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df_tuples_w_names = [[pl.read_parquet(f\"./all_features/{GENERATE_FOR}_ppmi_order_cart.pqt\"),\n",
    "                            \"order_vs_cart\", [2]],\n",
    "                            [pl.read_parquet(f\"./all_features/{GENERATE_FOR}_ppmi_click_cart.pqt\"),\n",
    "                            \"click_vs_cart\", [0]],\n",
    "                           [pl.read_parquet(f\"./all_features/{GENERATE_FOR}_ppmi_cart_cart.pqt\"),\n",
    "                            \"cart_vs_cart\", [1]],\n",
    "                           [pl.read_parquet(f\"./all_features/{GENERATE_FOR}_ppmi_click_order.pqt\"),\n",
    "                            \"click_vs_order\", [0]],\n",
    "                           [pl.read_parquet(f\"./all_features/{GENERATE_FOR}_ppmi_order_order.pqt\"),\n",
    "                            \"order_vs_order\", [2]],\n",
    "                           [pl.read_parquet(f\"./all_features/{GENERATE_FOR}_ppmi_cart_order.pqt\"),\n",
    "                            \"cart_vs_order\", [1]]\n",
    "                          ]\n",
    "\n",
    "generate_candidate_history_pair_score_features(input_val_df_path=val_path,\n",
    "                                               score_df_tuples_w_names=score_df_tuples_w_names,\n",
    "                                               score_col=\"PPMI\",\n",
    "                                               scoring_name=\"ppmi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7826c4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df_tuples_w_names = [[pl.read_parquet(f\"./all_features/{GENERATE_FOR}_ppmi_order_cart.pqt\"),\n",
    "                            \"order_vs_cart\", None],\n",
    "                            [pl.read_parquet(f\"./all_features/{GENERATE_FOR}_ppmi_click_cart.pqt\"),\n",
    "                            \"click_vs_cart\", None],\n",
    "                           [pl.read_parquet(f\"./all_features/{GENERATE_FOR}_ppmi_cart_cart.pqt\"),\n",
    "                            \"cart_vs_cart\", None],\n",
    "                           [pl.read_parquet(f\"./all_features/{GENERATE_FOR}_ppmi_click_order.pqt\"),\n",
    "                            \"click_vs_order\", None],\n",
    "                           [pl.read_parquet(f\"./all_features/{GENERATE_FOR}_ppmi_order_order.pqt\"),\n",
    "                            \"order_vs_order\", None],\n",
    "                           [pl.read_parquet(f\"./all_features/{GENERATE_FOR}_ppmi_cart_order.pqt\"),\n",
    "                            \"cart_vs_order\", None]\n",
    "                          ]\n",
    "\n",
    "generate_candidate_history_pair_score_features(input_val_df_path=val_path,\n",
    "                                               score_df_tuples_w_names=score_df_tuples_w_names,\n",
    "                                               score_col=\"PPMI\",\n",
    "                                               scoring_name=\"ppmi_all_history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a80ce4",
   "metadata": {},
   "source": [
    "### Common Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12a0c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_aggregator(input_df,\n",
    "                        group_cols=[],\n",
    "                        wanted_cols=[]):\n",
    "    return_df = input_df.groupby(group_cols).agg(\n",
    "        {'hour':['mean', 'std'],\n",
    "         'dayofweek':['mean', 'std'],\n",
    "         'is_weekend':['mean']\n",
    "        })\n",
    "    return_df.columns = ['_'.join(group_cols) + '_' +  '_'.join(col) for col in return_df.columns]\n",
    "    return return_df\n",
    "\n",
    "def type_distribution_aggregator(input_df, \n",
    "                                 group_cols=[]):\n",
    "    return_df = input_df.groupby(group_cols)['type'].value_counts(normalize=True)\n",
    "    return_df = return_df.unstack('type')\n",
    "    return_df.columns = ['_'.join(group_cols) + '_type' + str(col) + \"_mean\" for col in return_df.columns]\n",
    "    return return_df\n",
    "\n",
    "def type_based_aggregator(input_df,\n",
    "                          group_cols=[],\n",
    "                          wanted_cols=[],\n",
    "                          aggregators=[]):\n",
    "    type_dfs = []\n",
    "    for type_id in range(3):\n",
    "        for aggregator in aggregators:\n",
    "            aggregator_df = aggregator(input_df[input_df.type==type_id].reset_index(drop=True),\n",
    "                                       group_cols=group_cols,\n",
    "                                       wanted_cols=wanted_cols)\n",
    "            aggregator_df.columns = [\"type\" + str(type_id) + \"_\" + col for col in aggregator_df.columns]\n",
    "        type_dfs.append(aggregator_df)\n",
    "        \n",
    "    return pd.concat(type_dfs, axis=1)\n",
    "\n",
    "def existence_amount_aggregator(input_df,\n",
    "                                 group_cols=[],\n",
    "                                wanted_cols=[],\n",
    "                                return_counts=False):\n",
    "    \n",
    "    return_df = input_df.groupby(group_cols).agg({col:[\"count\"] for col in wanted_cols})\n",
    "    return_df.columns = ['_'.join(group_cols) + '_' +  '_'.join(col) for col in return_df.columns]\n",
    "        \n",
    "    return_df['_'.join(group_cols) + '_cnt_pct_rank'] =\\\n",
    "            return_df[return_df.columns[0]].rank(pct=True).astype(np.float32)\n",
    "    \n",
    "    count_cols = list(return_df.columns[:1])\n",
    "    \n",
    "    for count_col in count_cols:  \n",
    "        return_df[count_col.replace(\"count\", \"existed\")] = (return_df[count_col]>0).astype(int)\n",
    "        return_df[count_col.replace(\"count\", \"existed_multiple\")] = (return_df[count_col]>1).astype(int)\n",
    "        if return_counts:\n",
    "            return_df[count_col.replace(\"count\", \"existed_times\")] = (return_df[count_col]).astype(int)\n",
    "    \n",
    "    return_df = return_df[[col for col in return_df.columns if (\"count\" not in col)]]\n",
    "    \n",
    "    return return_df\n",
    "\n",
    "def nunique_aggregator(input_df,\n",
    "                       group_cols=[],\n",
    "                       wanted_cols=[]):\n",
    "    \n",
    "    return_df = input_df.groupby(group_cols).agg({col:[\"nunique\"] for col in wanted_cols})\n",
    "    return_df.columns = ['_'.join(group_cols) + '_' +  '_'.join(col) for col in return_df.columns]\n",
    "    \n",
    "    return_df['_'.join(group_cols) + '_nunq_pct_rank'] =\\\n",
    "            return_df[return_df.columns[0]].rank(pct=True).astype(np.float32)\n",
    "        \n",
    "\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966bdf62",
   "metadata": {},
   "source": [
    "### Item Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436a2a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_interacted_by_sessions_perc(input_df,\n",
    "              group_cols=[\"aid\"],\n",
    "              wanted_cols=[]\n",
    "             ):\n",
    "    return_feature_list = []\n",
    "    copy_df = input_df.copy()\n",
    "    session_count = copy_df.session.nunique()\n",
    "    \n",
    "    return_df = 2\n",
    "    \n",
    "    for type_str in tqdm(type_labels.keys()):\n",
    "        colname = f\"session_toitem_{type_str}_perc\"\n",
    "        type_df = copy_df[copy_df.type==type_labels[type_str]].groupby([\"aid\", \"session\"])[\"ts\"].count()    \n",
    "        type_df = type_df.reset_index().groupby(\"aid\")[\"session\"].count().rename(colname).to_frame() / session_count\n",
    "        if isinstance(return_df, int):\n",
    "            return_df = type_df.copy()\n",
    "        else:\n",
    "            return_df = return_df.merge(type_df, how=\"outer\", on=\"aid\")\n",
    "            \n",
    "    return return_df.fillna(0.)\n",
    "\n",
    "\n",
    "def item_lastweek_features(input_df,\n",
    "              group_cols=[\"aid\"],\n",
    "              wanted_cols=[]\n",
    "             ):\n",
    "    \n",
    "    input_df[\"ts\"] = pd.to_datetime(input_df[\"ts\"], unit=\"s\")\n",
    "    input_df[\"week\"] = input_df[\"ts\"].dt.week\n",
    "    \n",
    "    all_group_ids = pd.MultiIndex.from_product([input_df.aid.unique(),\n",
    "                                                input_df.week.unique(),\n",
    "                                                [0,1,2]],\n",
    "                                               names=['aid', 'week', 'type'])\n",
    "\n",
    "    return_dfs = []\n",
    "    \n",
    "    for aggfunc in [\"count\", \"nunique\"]:\n",
    "        grouped = input_df.groupby(['aid', 'week', 'type'])[\"session\"].agg(aggfunc).rename(aggfunc)\n",
    "        grouped = grouped.reindex(all_group_ids, fill_value=0).reset_index()\n",
    "\n",
    "        aid_lastweek_occ_ratio = (grouped.groupby([\"aid\", \"type\"])[aggfunc].last() /\\\n",
    "                                grouped.groupby([\"aid\", \"type\"])[aggfunc].sum()).fillna(0.).unstack('type')\n",
    "        aid_lastweek_occ_ratio.columns = [ f\"type_{col}_lastweek_{aggfunc}_occ_ratio\" for col in aid_lastweek_occ_ratio]        \n",
    "        return_dfs.append(aid_lastweek_occ_ratio)\n",
    "        \n",
    "        aid_lastweek_occ_amount = grouped.groupby([\"aid\", \"type\"])[aggfunc].last().fillna(0.).unstack('type')\n",
    "        aid_lastweek_occ_amount.columns = [ f\"type_{col}_lastweek_{aggfunc}_occ_amount\" for col in aid_lastweek_occ_amount]        \n",
    "        return_dfs.append(aid_lastweek_occ_amount)\n",
    "        \n",
    "        grouped[\"pct_change\"] = grouped.groupby([\"aid\", \"type\"])[aggfunc].pct_change()\n",
    "\n",
    "        aid_lastweek_pct_change = grouped.groupby([\"aid\", \"type\"])[\"pct_change\"].last().fillna(-999.).\\\n",
    "                    replace([np.inf, -np.inf], -999.).unstack('type')\n",
    "        aid_lastweek_pct_change.columns = [ f\"type_{col}_lastweek_{aggfunc}_pct_change\" for col in aid_lastweek_pct_change]\n",
    "        return_dfs.append(aid_lastweek_pct_change)\n",
    "        \n",
    "    return pd.concat(return_dfs, axis=1)\n",
    "\n",
    "def item_recurrent_signal(input_df,\n",
    "                          group_cols=[],\n",
    "                          wanted_cols=[]):\n",
    "    \n",
    "    item_recurrent_signal_df = input_df.groupby(['aid','session'])[\"ts\"].nunique()\\\n",
    "            .rename(\"recurrent_session_acts_per_item\").reset_index()\n",
    "    \n",
    "    item_recurrent_signal_df = item_recurrent_signal_df.groupby('aid').agg({\n",
    "        'recurrent_session_acts_per_item': ['mean'],\n",
    "    })\n",
    "    item_recurrent_signal_df.columns = ['aid_' +  '_'.join(col) for col in item_recurrent_signal_df.columns]\n",
    "\n",
    "    return item_recurrent_signal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2f2503",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = pd.concat([\n",
    "    existence_amount_aggregator(item_df,\n",
    "                                group_cols=[\"aid\"],\n",
    "                                wanted_cols=[\"session\", \"aid\"]),\n",
    "    nunique_aggregator(item_df,\n",
    "                       group_cols=[\"aid\"],\n",
    "                       wanted_cols=[\"session\"]),\n",
    "    datetime_aggregator(item_df,\n",
    "                        group_cols=[\"aid\"]),\n",
    "    type_distribution_aggregator(item_df,\n",
    "                                 group_cols=[\"aid\"]),\n",
    "    item_interacted_by_sessions_perc(item_df),\n",
    "    item_lastweek_features(item_df),\n",
    "    item_recurrent_signal(item_df),\n",
    "    type_based_aggregator(item_df,\n",
    "                          group_cols=[\"aid\"],\n",
    "                          wanted_cols=[\"aid\", \"session\"],\n",
    "                          aggregators=[datetime_aggregator,\n",
    "                                       nunique_aggregator,\n",
    "                                       existence_amount_aggregator,\n",
    "                                      item_recurrent_signal])\n",
    "], axis=1)\n",
    "\n",
    "item_features = reduce_memory(item_features)\n",
    "\n",
    "item_features.to_parquet(f'./all_features/{GENERATE_FOR}_item_features.pqt')\n",
    "\n",
    "print(\"Item features are created!\")\n",
    "\n",
    "del item_features; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b95c571",
   "metadata": {},
   "source": [
    "### User Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b85dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def session_len(input_df,\n",
    "                group_cols=[\"session\"],\n",
    "                wanted_cols=[],\n",
    "                return_min_max=False\n",
    "               ):\n",
    "    return_df = input_df[group_cols + [\"ts\"]].copy()\n",
    "    return_df = return_df.groupby(group_cols).agg({\"ts\":[\"min\", \"max\"]})\n",
    "    return_df.columns = [\"session_start\", \"session_end\"]\n",
    "    return_df[\"session_len\"] = return_df[\"session_end\"] - return_df[\"session_start\"]\n",
    "    \n",
    "    if return_min_max:\n",
    "        return return_df\n",
    "    else:\n",
    "        return return_df[[\"session_len\"]]\n",
    "    \n",
    "def partial_session_features(input_df,\n",
    "                        group_cols=[\"session\"],\n",
    "                        wanted_cols=[]\n",
    "           ):\n",
    "    \n",
    "    return_feature_list = []\n",
    "    \n",
    "    return_df = input_df.copy()\n",
    "    return_df[\"ts_diff_gt_thr\"] = (return_df.groupby(\"session\")[\"ts\"].diff() > 60*60*6).astype(int)\n",
    "    return_df[\"partial_session_id\"] = return_df.groupby(\"session\")[\"ts_diff_gt_thr\"].cumsum()\n",
    "\n",
    "    max_partial_sessions_per_session = return_df.groupby(\"session\")[\"partial_session_id\"].\\\n",
    "                max().rename(\"max_partial_session_id\")\n",
    "    return_feature_list.append(\"max_partial_session_id\")\n",
    "    \n",
    "    return_df = return_df.merge(max_partial_sessions_per_session, how=\"left\", on=\"session\")\n",
    "\n",
    "    #########\n",
    "    \n",
    "    mean_nunq_items = return_df.groupby([\"session\", \"partial_session_id\"])[\"aid\"].nunique().\\\n",
    "        reset_index().groupby(\"session\")[\"aid\"].mean().rename(\"partial_mean_nunique_aid\")\n",
    "    return_df = return_df.merge(mean_nunq_items, how=\"left\", on=\"session\")\n",
    "    return_feature_list.append(\"partial_mean_nunique_aid\")\n",
    "\n",
    "    #########\n",
    "    \n",
    "    mean_items = return_df.groupby([\"session\", \"partial_session_id\"])[\"aid\"].count().\\\n",
    "        reset_index().groupby(\"session\")[\"aid\"].mean().rename(\"partial_mean_count_aid\")\n",
    "    return_df = return_df.merge(mean_items, how=\"left\", on=\"session\")\n",
    "    return_feature_list.append(\"partial_mean_count_aid\")\n",
    "\n",
    "    #########\n",
    "    \n",
    "    for type_str in type_labels.keys():\n",
    "        mean_type_colname = f\"partial_mean_{type_str}\"\n",
    "        mean_type = return_df[return_df.type==type_labels[type_str]].groupby([\"session\", \"partial_session_id\"])[\"ts\"].count().\\\n",
    "            reset_index().groupby(\"session\")[\"ts\"].mean().rename(mean_type_colname)\n",
    "        return_df = return_df.merge(mean_type, how=\"left\", on=\"session\").fillna(0)\n",
    "        return_feature_list.append(mean_type_colname)    \n",
    "\n",
    "        #########\n",
    "        \n",
    "        mean_type_colname = f\"partial_mean_tsdiff_{type_str}\"\n",
    "        return_df.loc[return_df.type==type_labels[type_str], \"ts_diff\"] =\\\n",
    "            return_df[return_df.type==type_labels[type_str]].groupby([\"session\", \"partial_session_id\"])[\"ts\"].\\\n",
    "                    diff().rename(\"ts_diff\")\n",
    "        \n",
    "        ts_meandiff = return_df[return_df.type==type_labels[type_str]].groupby(\"session\")[\"ts_diff\"].mean().rename(mean_type_colname)\n",
    "        return_df = return_df.merge(ts_meandiff, how=\"left\", on=\"session\").fillna(-999)\n",
    "        return_feature_list.append(mean_type_colname)    \n",
    "        \n",
    "        #########\n",
    "        \n",
    "    return return_df[[\"session\"] + return_feature_list].groupby(\"session\").first()\n",
    "\n",
    "def order_size_stats(input_df,\n",
    "                        group_cols=[\"session\"],\n",
    "                        wanted_cols=[]\n",
    "           ):\n",
    "    return_df = input_df[input_df.type==2].reset_index().copy()\n",
    "    \n",
    "    order_size_stat_df = return_df.groupby([\"session\", \"ts\"]).agg({\"aid\":[\"nunique\"]})\n",
    "    order_size_stat_df.columns = [\"aid_in_orders_nunique\"]\n",
    "    order_size_stat_df = order_size_stat_df.reset_index().groupby(\"session\").agg({\n",
    "        \"aid_in_orders_nunique\": [\"min\", \"max\", \"mean\"]\n",
    "    })\n",
    "    order_size_stat_df.columns = ['_'.join(group_cols) + '_' +  '_'.join(col) for col in order_size_stat_df.columns]\n",
    "    return order_size_stat_df\n",
    "\n",
    "def user_action_conversion_ratios(input_df,\n",
    "                                  group_cols=[\"session\"],\n",
    "                                  wanted_cols=[]\n",
    "                                 ):\n",
    "    return_df = input_df.copy()\n",
    "    \n",
    "    for i in range(3):\n",
    "        return_df.loc[return_df.type==i, f\"type_{i}\"] = 1\n",
    "    return_df.fillna(0., inplace=True)\n",
    "    \n",
    "    return_df = return_df.groupby([\"session\", \"aid\"]).agg({\"type_0\": \"max\",\n",
    "                                                 \"type_1\": \"max\",\n",
    "                                                 \"type_2\": \"max\"}).reset_index()\n",
    "    return_df[\"session_click_cart_relation\"] = ((return_df[\"type_0\"] == 1.) & return_df[\"type_1\"] == 1.).astype(int)\n",
    "    return_df[\"session_click_order_relation\"] = ((return_df[\"type_0\"] == 1.) & return_df[\"type_2\"] == 1.).astype(int)\n",
    "    return_df[\"session_cart_order_relation\"] = ((return_df[\"type_1\"] == 1.) & return_df[\"type_2\"] == 1.).astype(int)\n",
    "    \n",
    "    return_df = return_df.groupby([\"session\"]).agg({\n",
    "        \"session_click_cart_relation\":\"mean\",\n",
    "        \"session_click_order_relation\":\"mean\",\n",
    "        \"session_cart_order_relation\":\"mean\",\n",
    "    })\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397c408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features = pd.concat([\n",
    "    existence_amount_aggregator(user_df,\n",
    "                                group_cols=[\"session\"],\n",
    "                                wanted_cols=[\"session\", \"aid\"]),\n",
    "    session_len(user_df),\n",
    "    nunique_aggregator(user_df,\n",
    "                       group_cols=[\"session\"],\n",
    "                       wanted_cols=[\"aid\"]),\n",
    "    datetime_aggregator(user_df,\n",
    "                        group_cols=[\"session\"]),\n",
    "    type_distribution_aggregator(user_df,\n",
    "                                 group_cols=[\"session\"]),\n",
    "    partial_session_features(user_df),\n",
    "    order_size_stats(user_df),\n",
    "    user_action_conversion_ratios(user_df),\n",
    "    type_based_aggregator(user_df,\n",
    "                          group_cols=[\"session\"],\n",
    "                          wanted_cols=[\"aid\", \"session\"],\n",
    "                          aggregators=[datetime_aggregator,\n",
    "                                       session_len,\n",
    "                                       nunique_aggregator,\n",
    "                                       existence_amount_aggregator])\n",
    "], axis=1)\n",
    "\n",
    "user_features = reduce_memory(user_features)\n",
    "\n",
    "user_features.to_parquet(f'./all_features/{GENERATE_FOR}_user_features.pqt')\n",
    "\n",
    "print(\"User features are created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06276e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "del user_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62810d2d",
   "metadata": {},
   "source": [
    "### User-Item Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a2d28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_last_aid_of_the_session(input_df,\n",
    "                               group_cols=[\"session\", \"aid\"],\n",
    "                               wanted_cols=[]\n",
    "                              ):\n",
    "    \n",
    "    return_df = input_df[group_cols].copy()\n",
    "    return_df[\"is_aid_interacted_last\"] = 0\n",
    "    return_df.loc[return_df.session.shift(-1) != return_df.session, \"is_aid_interacted_last\"] = 1\n",
    "    return_df = return_df.groupby(group_cols).agg({\"is_aid_interacted_last\":[\"max\"]})\n",
    "    return_df.columns = [\"is_aid_interacted_last_in_session\"]\n",
    "    return return_df\n",
    "\n",
    "def aid_session_ts_offsets(input_df,\n",
    "                group_cols=[\"session\", \"aid\"],\n",
    "                wanted_cols=[]):\n",
    "    session_lens = session_len(input_df,\n",
    "                               return_min_max=True).reset_index()\n",
    "    return_df = input_df[group_cols + [\"ts\"]].copy()\n",
    "    return_df = return_df.groupby(group_cols).agg({\"ts\":[\"last\"]})\n",
    "    return_df.columns = [\"session_aid_last_ts\"]\n",
    "    return_df.reset_index(inplace=True)\n",
    "    return_df = return_df.merge(session_lens, how=\"left\", on=\"session\")\n",
    "    return_df[\"aid_ts_session_end_offset\"] = return_df[\"session_end\"] - return_df[\"session_aid_last_ts\"]\n",
    "    return_df[\"aid_ts_session_start_offset\"] = return_df[\"session_aid_last_ts\"] - return_df[\"session_start\"]\n",
    "\n",
    "    return_df = return_df[group_cols + [\"aid_ts_session_end_offset\", \"aid_ts_session_start_offset\"]].set_index(group_cols)\n",
    "    return return_df\n",
    "\n",
    "def reverse_order_of_aid_for_session(input_df,\n",
    "                               group_cols=[\"session\", \"aid\"],\n",
    "                               wanted_cols=[]\n",
    "                              ):\n",
    "    \n",
    "    return_df = input_df[group_cols].copy()\n",
    "    \n",
    "    return_df.loc[:, \"session_action_order\"] = return_df.groupby(\"session\")[\"aid\"].cumcount()\n",
    "\n",
    "    session_aid_counts = return_df.groupby(\"session\")[\"aid\"].count()\\\n",
    "                        .rename(\"session_action_count\").reset_index()\n",
    "    return_df = return_df.merge(session_aid_counts, how=\"left\", on=\"session\")\n",
    "    return_df[\"session_action_order\"] = return_df[\"session_action_count\"] - return_df[\"session_action_order\"]\n",
    "    \n",
    "    return_df = return_df.groupby(group_cols)[\"session_action_order\"].min().rename(\"session_action_last_order\").to_frame()\n",
    "    \n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede33468",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_int_features = pd.concat([\n",
    "    existence_amount_aggregator(user_item_int_df,\n",
    "                                group_cols=[\"session\", \"aid\"],\n",
    "                                wanted_cols=[\"aid\"],\n",
    "                                return_counts=True),\n",
    "    reverse_order_of_aid_for_session(user_item_int_df),\n",
    "    aid_session_ts_offsets(user_item_int_df),\n",
    "#     nunique_aggregator(user_df,\n",
    "#                        group_cols=[\"session\"],\n",
    "#                        wanted_cols=[\"aid\"]),\n",
    "    datetime_aggregator(user_item_int_df,\n",
    "                        group_cols=['session', 'aid']),\n",
    "    type_distribution_aggregator(user_item_int_df,\n",
    "                                 group_cols=['session', 'aid']),\n",
    "    type_based_aggregator(user_item_int_df,\n",
    "                          group_cols=['session', 'aid'],\n",
    "                          wanted_cols=[\"aid\"],\n",
    "                          aggregators=[datetime_aggregator,\n",
    "                                       reverse_order_of_aid_for_session,\n",
    "                                       aid_session_ts_offsets,\n",
    "#                                        nunique_aggregator,\n",
    "                                       existence_amount_aggregator])\n",
    "], axis=1)\n",
    "\n",
    "user_item_int_features = reduce_memory(user_item_int_features)\n",
    "\n",
    "user_item_int_features.to_parquet(f'./all_features/{GENERATE_FOR}_user_item_int_features.pqt')\n",
    "\n",
    "print(\"User-Item Interaction features are created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a360283",
   "metadata": {},
   "outputs": [],
   "source": [
    "del user_item_int_features\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a850637",
   "metadata": {},
   "outputs": [],
   "source": [
    "del item_df, train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03248e26",
   "metadata": {},
   "source": [
    "## Merging Features w/ Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ce29ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading item features...\")\n",
    "item_features = pl.scan_parquet(f'./all_features/{GENERATE_FOR}_item_features.pqt')\n",
    "print(\"Reading user features...\")\n",
    "user_features = pl.scan_parquet(f'./all_features/{GENERATE_FOR}_user_features.pqt')\n",
    "print(\"Reading user-item interaction features...\")\n",
    "user_item_int_features = pl.scan_parquet(f'./all_features/{GENERATE_FOR}_user_item_int_features.pqt')\n",
    "\n",
    "val_df = pl.scan_parquet(val_path)\n",
    "    \n",
    "for type_str in tqdm(list(type_labels.keys())):\n",
    "    \n",
    "    covisit_feature_df = pl.scan_parquet(f'../raw_data/{GENERATE_FOR}_covisitation_features/covisitation_features_{type_str}_{CANDIDATE_COUNT}candidates.pqt')\n",
    "    all_clicks_covisit_feature_df = pl.scan_parquet(f'../raw_data/{GENERATE_FOR}_covisitation_features/all_clicks_covisitation_features.pqt')\n",
    "    all_cart_covisit_feature_df = pl.scan_parquet(f'../raw_data/{GENERATE_FOR}_covisitation_features/all_carts_orders_covisitation_features.pqt')\n",
    "    all_buy2buy_covisit_feature_df = pl.scan_parquet(f'../raw_data/{GENERATE_FOR}_covisitation_features/all_buy2buy_covisitation_features.pqt')\n",
    "    \n",
    "    ppmi_all_feature_df = pl.scan_parquet(f'../raw_data/{GENERATE_FOR}_ppmi_all_history_features/ppmi_all_history_features_{type_str}_{CANDIDATE_COUNT}candidates.pqt')\n",
    "\n",
    "    \n",
    "#     w2v_feature_df = pl.scan_parquet(f'../raw_data/{GENERATE_FOR}_word2vec_features/word2vec_features_{type_str}_{CANDIDATE_COUNT}candidates.pqt')\n",
    "\n",
    "    pf = ParquetFile(f\"./candidate_data/{GENERATE_FOR}_{CANDIDATE_COUNT}candidates_{type_str}.parquet\")\n",
    "    chunk = 10_000_000\n",
    "    \n",
    "    total_candidate_df = 0\n",
    "    \n",
    "    \n",
    "    for batch_i, batch in tqdm(enumerate(pf.iter_batches(batch_size = chunk))):\n",
    "        candidate_df = batch.to_pandas()\n",
    "        candidate_df = pl.from_pandas(candidate_df)  \n",
    "\n",
    "        rank_repeater = np.hstack([list(range(1,CANDIDATE_COUNT+1)) for i in range(int(len(candidate_df)/CANDIDATE_COUNT))])\n",
    "        candidate_df = candidate_df.with_column(pl.Series(name=\"candidate_rank\", values=rank_repeater))\n",
    "        del rank_repeater;gc.collect()\n",
    "\n",
    "        candidate_df = candidate_df.join(covisit_feature_df, on=['session',\n",
    "                                                                 'aid'], how='left').fill_null(-1)\n",
    "        candidate_df = candidate_df.unique()\n",
    "\n",
    "        candidate_df = candidate_df.join(ppmi_all_feature_df, on=['session',\n",
    "                                                              'aid'], how='left').fill_null(-1)\n",
    "        candidate_df = candidate_df.unique()\n",
    "        \n",
    "        candidate_df = candidate_df.join(all_clicks_covisit_feature_df, on=['aid'], how='left').fill_null(-1)\n",
    "        candidate_df = candidate_df.join(all_cart_covisit_feature_df, on=['aid'], how='left').fill_null(-1)\n",
    "        candidate_df = candidate_df.join(all_buy2buy_covisit_feature_df, on=['aid'], how='left').fill_null(-1)\n",
    "        \n",
    "        candidate_df = candidate_df.unique()\n",
    "\n",
    "        #print('Candidate Rank Features, Done...')\n",
    "        candidate_df = candidate_df.join(item_features, on='aid', how='left').fill_null(-1)\n",
    "        #print('Item Features, Done...')\n",
    "        candidate_df = candidate_df.join(user_features, on='session', how='left').fill_null(-1)\n",
    "        #print('User Features, Done...')\n",
    "        candidate_df = candidate_df.join(user_item_int_features,\n",
    "                                          on=['session', 'aid'],\n",
    "                                          how='left').fill_null(-1)\n",
    "        #print('User-Item Features, Done...')\n",
    "        tar = pd.read_parquet('./splitted_raw_data/val_labels.parquet')\n",
    "        tar = tar.loc[ tar['type'] == type_str ]\n",
    "        aids = tar.ground_truth.explode().rename('aid')\n",
    "        tar = tar[['session']]\n",
    "        tar = tar.merge(aids, left_index=True, right_index=True, how='left')\n",
    "        tar['label'] = 1\n",
    "        #print('Extract Labels, Done...')\n",
    "        \n",
    "        tar = pl.from_pandas(tar)\n",
    "        \n",
    "        candidate_df = candidate_df.join(tar, on=['session','aid'], how='left').fill_null(0)\n",
    "        candidate_df = candidate_df.unique()\n",
    "        candidate_df.write_parquet(f'./candidated_features/{GENERATE_FOR}_{type_str}_all_data_{CANDIDATE_COUNT}candidates_p{batch_i}.pqt')\n",
    "        \n",
    "        del candidate_df,tar,aids;gc.collect()\n",
    "        \n",
    "    del covisit_feature_df;gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3695.647257,
   "end_time": "2022-11-28T19:50:53.428271",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-28T18:49:17.781014",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
