{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19803c5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T18:49:25.672561Z",
     "iopub.status.busy": "2022-11-28T18:49:25.671835Z",
     "iopub.status.idle": "2022-11-28T18:49:28.346781Z",
     "shell.execute_reply": "2022-11-28T18:49:28.345442Z"
    },
    "papermill": {
     "duration": 2.696152,
     "end_time": "2022-11-28T18:49:28.349900",
     "exception": false,
     "start_time": "2022-11-28T18:49:25.653748",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "VER = 6\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import os, sys, pickle, glob, gc\n",
    "from collections import Counter\n",
    "import cudf, itertools\n",
    "print('We will use RAPIDS version',cudf.__version__)\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(nb_workers=4, progress_bar=True, use_memory_fs=False)\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "from pyarrow.parquet import ParquetFile\n",
    "import pyarrow as pa \n",
    "\n",
    "from catboost import CatBoostRanker, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf93613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_negative_session(df,target='label'):\n",
    "    true_df = df.groupby('session')[target].agg('sum') > 0\n",
    "    session = pd.DataFrame(true_df[true_df]).reset_index()['session']\n",
    "    df = df.merge(session, how = 'inner', on = 'session')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2505537",
   "metadata": {
    "papermill": {
     "duration": 0.020472,
     "end_time": "2022-11-28T18:57:43.433117",
     "exception": false,
     "start_time": "2022-11-28T18:57:43.412645",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b81b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_labels = {'clicks':0, 'carts':1, 'orders':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b29238",
   "metadata": {},
   "outputs": [],
   "source": [
    "CANDIDATE_COUNT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6758ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_FOR = \"kaggle\" # \"kaggle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2874c301",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sessions = np.load(\"./splitted_raw_data/val_sessions_for_train.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98185d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./models/model_iters.json\", \"r\") as read_file:\n",
    "    model_iters = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e510d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb75eb40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subs = []\n",
    "\n",
    "for type_str in tqdm(list(type_labels.keys())):\n",
    "        \n",
    "    batches = sorted(glob.glob(f\"./candidated_features/{RUN_FOR}_{type_str}_all_data_{CANDIDATE_COUNT}candidates_p*.pqt\"))\n",
    "\n",
    "    model_paths = sorted(glob.glob(f\"./models/XGB_{CANDIDATE_COUNT}candidates_fold*_{type_str}.xgb\"))\n",
    "\n",
    "    all_predictions = []\n",
    "    \n",
    "    for batch in tqdm(batches):\n",
    "        whole_df = pd.read_parquet(batch)\n",
    "\n",
    "        if RUN_FOR == \"local\":\n",
    "#             whole_df = remove_negative_session(whole_df).reset_index(drop=True)\n",
    "            whole_df = whole_df[~whole_df.session.isin(train_sessions)].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Processing {len(whole_df)} rows...\")\n",
    "\n",
    "        CHUNK_SIZE = 1_500_000\n",
    "        \n",
    "        for chunk_num in range(len(whole_df) // CHUNK_SIZE + 1):\n",
    "            start_index = chunk_num*CHUNK_SIZE\n",
    "            end_index = min(chunk_num*CHUNK_SIZE + CHUNK_SIZE, len(whole_df))\n",
    "            print(start_index, end_index)\n",
    "            chunk_df = whole_df.iloc[start_index:end_index]\n",
    "\n",
    "            FEATURES = chunk_df.columns[2 : -1]\n",
    "            dtest = xgb.DMatrix(data=chunk_df[FEATURES])\n",
    "\n",
    "            preds = [] \n",
    "\n",
    "            for model_path in model_paths:\n",
    "                model = xgb.Booster()\n",
    "                model.load_model(model_path)\n",
    "                model.set_param({'predictor': 'gpu_predictor'})\n",
    "                preds.append(model.predict(dtest))\n",
    "                \n",
    "            preds = np.mean(preds, axis=0)\n",
    "            \n",
    "            predictions = chunk_df[['session','aid']].copy()\n",
    "            predictions['pred'] = preds\n",
    "            all_predictions.append(predictions)\n",
    "        \n",
    "    all_predictions = pd.concat(all_predictions, ignore_index=True)\n",
    "    \n",
    "    all_predictions = all_predictions.sort_values(['session','pred'],\n",
    "                                                  ascending=[True,False]).reset_index(drop=True)\n",
    "    \n",
    "    all_predictions.to_parquet(f\"../raw_data/soft_scores/{RUN_FOR}_{type_str}_soft_scores.parquet\")\n",
    "        \n",
    "    all_predictions['n'] = all_predictions.groupby('session').aid.cumcount().astype('int8')\n",
    "    all_predictions = all_predictions.loc[all_predictions.n<20]\n",
    "\n",
    "    sub = all_predictions.groupby('session').aid.apply(list)\n",
    "    sub = sub.to_frame().reset_index()\n",
    "    sub.item = sub.aid.apply(lambda x: \" \".join(map(str,x)))\n",
    "    sub.columns = ['session_type','labels']\n",
    "    sub.session_type = sub.session_type.astype('str') + '_' + type_str\n",
    "\n",
    "    subs.append(sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988d59c2",
   "metadata": {},
   "source": [
    "## Local Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b4d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sub = pd.concat(subs, ignore_index=True)\n",
    "final_sub.sort_values(by=\"session_type\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "if RUN_FOR == \"local\":\n",
    "    # COMPUTE METRIC\n",
    "    score = 0\n",
    "    weights = {'clicks': 0.10, 'carts': 0.30, 'orders': 0.60}\n",
    "    for t in [\n",
    "        'clicks',\n",
    "        'carts',\n",
    "        'orders'\n",
    "    ]:\n",
    "        sub = final_sub.loc[final_sub.session_type.str.contains(t)].copy()\n",
    "        sub['session'] = sub.session_type.apply(lambda x: int(x.split('_')[0]))\n",
    "        test_labels = pd.read_parquet('./splitted_raw_data/val_labels.parquet')\n",
    "        test_labels = test_labels[~test_labels.session.isin(train_sessions)].reset_index(drop=True)\n",
    "        test_labels = test_labels.loc[test_labels['type']==t]\n",
    "        test_labels = test_labels.merge(sub, how='left', on=['session'])\n",
    "        test_labels['labels'] = test_labels['labels'].fillna(\"\").apply(list)\n",
    "        test_labels['hits'] = test_labels.apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n",
    "        test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0,20)\n",
    "        recall = test_labels['hits'].sum() / test_labels['gt_count'].sum()\n",
    "        score += weights[t]*recall\n",
    "        print(f'{t} recall =',recall)\n",
    "\n",
    "    print('=============')\n",
    "    print('Overall Recall =',score)\n",
    "    print('=============')\n",
    "\n",
    "elif RUN_FOR == \"kaggle\":\n",
    "    final_sub[\"labels\"] = final_sub.labels.apply(lambda x: \" \".join([str(elm) for elm in x]))\n",
    "    final_sub.to_csv(\"submission.csv.gz\", index=False, compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3695.647257,
   "end_time": "2022-11-28T19:50:53.428271",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-28T18:49:17.781014",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
