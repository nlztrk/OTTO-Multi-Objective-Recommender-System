{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19803c5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T18:49:25.672561Z",
     "iopub.status.busy": "2022-11-28T18:49:25.671835Z",
     "iopub.status.idle": "2022-11-28T18:49:28.346781Z",
     "shell.execute_reply": "2022-11-28T18:49:28.345442Z"
    },
    "papermill": {
     "duration": 2.696152,
     "end_time": "2022-11-28T18:49:28.349900",
     "exception": false,
     "start_time": "2022-11-28T18:49:25.653748",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use RAPIDS version 22.10.00a+392.g1558403753\n",
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "VER = 6\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import os, sys, pickle, glob, gc\n",
    "from collections import Counter\n",
    "import cudf, itertools\n",
    "print('We will use RAPIDS version',cudf.__version__)\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(nb_workers=4, progress_bar=True, use_memory_fs=False)\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "from pyarrow.parquet import ParquetFile\n",
    "import pyarrow as pa \n",
    "\n",
    "from catboost import CatBoostRanker, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccf93613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_negative_session(df,target='label'):\n",
    "    true_df = df.groupby('session')[target].agg('sum') > 0\n",
    "    session = pd.DataFrame(true_df[true_df]).reset_index()['session']\n",
    "    df = df.merge(session, how = 'inner', on = 'session')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2505537",
   "metadata": {
    "papermill": {
     "duration": 0.020472,
     "end_time": "2022-11-28T18:57:43.433117",
     "exception": false,
     "start_time": "2022-11-28T18:57:43.412645",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e70b81b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_labels = {'clicks':0, 'carts':1, 'orders':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67b29238",
   "metadata": {},
   "outputs": [],
   "source": [
    "CANDIDATE_COUNT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec6758ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_FOR = \"kaggle\" # \"kaggle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2874c301",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sessions = np.load(\"./splitted_raw_data/val_sessions_for_train.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d98185d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./models/model_iters.json\", \"r\") as read_file:\n",
    "    model_iters = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98e510d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'./models/XGB_100candidates_fold0_clicks.xgb': 900,\n",
       " './models/XGB_100candidates_fold1_clicks.xgb': 900,\n",
       " './models/XGB_100candidates_fold2_clicks.xgb': 900,\n",
       " './models/XGB_100candidates_fold3_clicks.xgb': 900,\n",
       " './models/XGB_100candidates_fold4_clicks.xgb': 900,\n",
       " './models/XGB_100candidates_fold0_carts.xgb': 400,\n",
       " './models/XGB_100candidates_fold1_carts.xgb': 400,\n",
       " './models/XGB_100candidates_fold2_carts.xgb': 400,\n",
       " './models/XGB_100candidates_fold3_carts.xgb': 400,\n",
       " './models/XGB_100candidates_fold4_carts.xgb': 400,\n",
       " './models/XGB_100candidates_fold0_orders.xgb': 400,\n",
       " './models/XGB_100candidates_fold1_orders.xgb': 400,\n",
       " './models/XGB_100candidates_fold2_orders.xgb': 400,\n",
       " './models/XGB_100candidates_fold3_orders.xgb': 400,\n",
       " './models/XGB_100candidates_fold4_orders.xgb': 400}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb75eb40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "477960dddd6843acb4febccca891a923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5373e10f03974d1cbbb30b981bbae1f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 7180300 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7180300\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2855292/2716664801.py:70: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  sub.item = sub.aid.apply(lambda x: \" \".join(map(str,x)))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee8cfeed3174c1f8d605449521d504b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 7180300 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7180300\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2855292/2716664801.py:70: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  sub.item = sub.aid.apply(lambda x: \" \".join(map(str,x)))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4762eb35d0a24058866b07277017e345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 7180300 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7180300\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n",
      "Processing 10000000 rows...\n",
      "0 1500000\n",
      "1500000 3000000\n",
      "3000000 4500000\n",
      "4500000 6000000\n",
      "6000000 7500000\n",
      "7500000 9000000\n",
      "9000000 10000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2855292/2716664801.py:70: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  sub.item = sub.aid.apply(lambda x: \" \".join(map(str,x)))\n"
     ]
    }
   ],
   "source": [
    "subs = []\n",
    "\n",
    "for type_str in tqdm(list(type_labels.keys())):\n",
    "        \n",
    "    batches = sorted(glob.glob(f\"./candidated_features/{RUN_FOR}_{type_str}_all_data_{CANDIDATE_COUNT}candidates_p*.pqt\"))\n",
    "\n",
    "    model_paths = sorted(glob.glob(f\"./models/XGB_{CANDIDATE_COUNT}candidates_fold*_{type_str}.xgb\"))\n",
    "#     model_paths = sorted(glob.glob(f\"./models/CB_{CANDIDATE_COUNT}candidates_fold*_{type_str}.cb\"))\n",
    "\n",
    "    all_predictions = []\n",
    "    \n",
    "    for batch in tqdm(batches):\n",
    "        whole_df = pd.read_parquet(batch)#.drop(labels=[\"candidate_rank\"], axis=1)\n",
    "#         whole_df.drop(columns=dropcols, axis=1, inplace=True)\n",
    "\n",
    "        #whole_df = whole_df.drop(labels=[col for col in whole_df.columns if col[:4]==\"type\"], axis=1)\n",
    "\n",
    "        if RUN_FOR == \"local\":\n",
    "#             whole_df = remove_negative_session(whole_df).reset_index(drop=True)\n",
    "            whole_df = whole_df[~whole_df.session.isin(train_sessions)].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Processing {len(whole_df)} rows...\")\n",
    "\n",
    "        CHUNK_SIZE = 1_500_000\n",
    "        \n",
    "        for chunk_num in range(len(whole_df) // CHUNK_SIZE + 1):\n",
    "            start_index = chunk_num*CHUNK_SIZE\n",
    "            end_index = min(chunk_num*CHUNK_SIZE + CHUNK_SIZE, len(whole_df))\n",
    "            print(start_index, end_index)\n",
    "            chunk_df = whole_df.iloc[start_index:end_index]#.drop(dropcols, 1)\n",
    "\n",
    "            FEATURES = chunk_df.columns[2 : -1]\n",
    "            dtest = xgb.DMatrix(data=chunk_df[FEATURES])\n",
    "    #         dtest = Pool(\n",
    "    #             data=whole_df[FEATURES]\n",
    "    #         )\n",
    "\n",
    "            preds = [] #np.zeros(len(chunk_df))\n",
    "\n",
    "            for model_path in model_paths:\n",
    "    #             target_it = int(model_path.split(\"_it\")[-1].split(\".xgb\")[0])\n",
    "    #             model = CatBoostRanker()\n",
    "    #             model.load_model(model_path)\n",
    "\n",
    "                model = xgb.Booster()\n",
    "                model.load_model(model_path)\n",
    "                model.set_param({'predictor': 'gpu_predictor'})\n",
    "\n",
    "                preds.append(model.predict(dtest#,\n",
    "#                                        iteration_range = (0, model_iters[model_path])\n",
    "                                      ))\n",
    "            preds = np.mean(preds, axis=0)\n",
    "            \n",
    "            predictions = chunk_df[['session','aid']].copy()\n",
    "            predictions['pred'] = preds\n",
    "            all_predictions.append(predictions)\n",
    "        \n",
    "    all_predictions = pd.concat(all_predictions, ignore_index=True)\n",
    "    \n",
    "    all_predictions = all_predictions.sort_values(['session','pred'],\n",
    "                                                  ascending=[True,False]).reset_index(drop=True)\n",
    "    \n",
    "    all_predictions.to_parquet(f\"../raw_data/soft_scores/{RUN_FOR}_{type_str}_soft_scores.parquet\")\n",
    "        \n",
    "    all_predictions['n'] = all_predictions.groupby('session').aid.cumcount().astype('int8')\n",
    "    all_predictions = all_predictions.loc[all_predictions.n<20]\n",
    "\n",
    "    sub = all_predictions.groupby('session').aid.apply(list)\n",
    "    sub = sub.to_frame().reset_index()\n",
    "    sub.item = sub.aid.apply(lambda x: \" \".join(map(str,x)))\n",
    "    sub.columns = ['session_type','labels']\n",
    "    sub.session_type = sub.session_type.astype('str') + '_' + type_str\n",
    "\n",
    "    subs.append(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1b64e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988d59c2",
   "metadata": {},
   "source": [
    "## Local Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09b4d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sub = pd.concat(subs, ignore_index=True)\n",
    "final_sub.sort_values(by=\"session_type\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "if RUN_FOR == \"local\":\n",
    "    # COMPUTE METRIC\n",
    "    score = 0\n",
    "    weights = {'clicks': 0.10, 'carts': 0.30, 'orders': 0.60}\n",
    "    for t in [\n",
    "        'clicks',\n",
    "        'carts',\n",
    "        'orders'\n",
    "    ]:\n",
    "        sub = final_sub.loc[final_sub.session_type.str.contains(t)].copy()\n",
    "        sub['session'] = sub.session_type.apply(lambda x: int(x.split('_')[0]))\n",
    "    #     sub.labels = sub.labels.apply(lambda x: [int(i) for i in x.split(' ')[:20]])\n",
    "        test_labels = pd.read_parquet('./splitted_raw_data/val_labels.parquet')\n",
    "        test_labels = test_labels[~test_labels.session.isin(train_sessions)].reset_index(drop=True)\n",
    "        test_labels = test_labels.loc[test_labels['type']==t]\n",
    "        test_labels = test_labels.merge(sub, how='left', on=['session'])\n",
    "        test_labels['labels'] = test_labels['labels'].fillna(\"\").apply(list)\n",
    "        test_labels['hits'] = test_labels.apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n",
    "        test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0,20)\n",
    "        recall = test_labels['hits'].sum() / test_labels['gt_count'].sum()\n",
    "        score += weights[t]*recall\n",
    "        print(f'{t} recall =',recall)\n",
    "\n",
    "    print('=============')\n",
    "    print('Overall Recall =',score)\n",
    "    print('=============')\n",
    "\n",
    "elif RUN_FOR == \"kaggle\":\n",
    "    final_sub[\"labels\"] = final_sub.labels.apply(lambda x: \" \".join([str(elm) for elm in x]))\n",
    "#     final_sub.to_csv(\"submission.csv\", index=False)\n",
    "    final_sub.to_csv(\"submission.csv.gz\", index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe853178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_df.session.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f91ff9f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mscore\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'score' is not defined"
     ]
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152fd19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels.ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737ca2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(model.feature_importances_, index=whole_df[FEATURES].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8a714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_str = \"clicks\"\n",
    "\n",
    "batches = sorted(glob.glob(f\"./candidated_features/{RUN_FOR}_{type_str}_all_data_{CANDIDATE_COUNT}candidates_p*.pqt\"))\n",
    "for batch in tqdm(batches):\n",
    "    whole_df = pd.read_parquet(batch)\n",
    "    if RUN_FOR == \"local\":\n",
    "        whole_df = remove_negative_session(whole_df).reset_index(drop=True)\n",
    "        whole_df = whole_df[~whole_df.session.isin(train_sessions)].reset_index(drop=True)\n",
    "        \n",
    "    FEATURES = whole_df.columns[2 : -1]\n",
    "    dtest = xgb.DMatrix(data=whole_df[FEATURES])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f8dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = sorted(glob.glob(f\"./models/XGB_{CANDIDATE_COUNT}candidates_fold*_{type_str}.xgb\"))[0]\n",
    "model = xgb.Booster()\n",
    "model.load_model(model_path)\n",
    "\n",
    "model.predict(dtest)\n",
    "\n",
    "imp_df = pd.Series(model.get_score(importance_type='gain'))\n",
    "imp_df.nlargest(30).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f4569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del whole_df, dtest, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c77a12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95553261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e9cf58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3695.647257,
   "end_time": "2022-11-28T19:50:53.428271",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-28T18:49:17.781014",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
